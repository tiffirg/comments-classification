{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import seaborn as sns\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "from datasets import load_dataset, Dataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation as PUNCT\n",
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    multilabel_confusion_matrix,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    hamming_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Направление</th><th>Факультет</th><th>ID студента</th><th>Оценка</th><th>Категория</th><th>Тег</th><th>Комментарий</th><th>Статус</th><th>Neutral</th><th>Positive</th><th>Negative</th><th>Exclamations</th><th>have_code</th><th>Neutral_NLP</th><th>Positive_NLP</th><th>Negative_NLP</th><th>Speech_NLP</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;C&quot;</td><td>113.0</td><td>1493.0</td><td>1.0</td><td>&quot;Видео&quot;</td><td>&quot;VP2&quot;</td><td>&quot;Видео лагает&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>1.00001</td><td>0.010663</td><td>0.00001</td><td>0.00001</td></tr><tr><td>&quot;C&quot;</td><td>113.0</td><td>5580.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Торгом Бабаян!…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5</td><td>false</td><td>0.437833</td><td>0.056662</td><td>0.140346</td><td>0.051855</td></tr><tr><td>&quot;E&quot;</td><td>126.0</td><td>5619.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3&quot;</td><td>&quot;Спасибо)&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.00001</td><td>0.00001</td><td>0.00001</td><td>1.00001</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>310.0</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2 E1&quot;</td><td>&quot;комментарий со…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.930468</td><td>0.025189</td><td>0.119213</td><td>0.000378</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>1913.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Жонибек, хочу …</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td><td>false</td><td>0.069552</td><td>0.217348</td><td>0.019134</td><td>0.507822</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 17)\n",
       "┌────────────┬───────────┬────────────┬────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ Направлени ┆ Факультет ┆ ID         ┆ Оценка ┆ … ┆ Neutral_N ┆ Positive_ ┆ Negative_ ┆ Speech_NL │\n",
       "│ е          ┆ ---       ┆ студента   ┆ ---    ┆   ┆ LP        ┆ NLP       ┆ NLP       ┆ P         │\n",
       "│ ---        ┆ f64       ┆ ---        ┆ f64    ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ str        ┆           ┆ f64        ┆        ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪════════════╪════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ C          ┆ 113.0     ┆ 1493.0     ┆ 1.0    ┆ … ┆ 1.00001   ┆ 0.010663  ┆ 0.00001   ┆ 0.00001   │\n",
       "│ C          ┆ 113.0     ┆ 5580.0     ┆ 5.0    ┆ … ┆ 0.437833  ┆ 0.056662  ┆ 0.140346  ┆ 0.051855  │\n",
       "│ E          ┆ 126.0     ┆ 5619.0     ┆ 5.0    ┆ … ┆ 0.00001   ┆ 0.00001   ┆ 0.00001   ┆ 1.00001   │\n",
       "│ E          ┆ 123.0     ┆ 310.0      ┆ 3.0    ┆ … ┆ 0.930468  ┆ 0.025189  ┆ 0.119213  ┆ 0.000378  │\n",
       "│ E          ┆ 123.0     ┆ 1913.0     ┆ 5.0    ┆ … ┆ 0.069552  ┆ 0.217348  ┆ 0.019134  ┆ 0.507822  │\n",
       "└────────────┴───────────┴────────────┴────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pl.read_csv(\"preprocessing.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_258, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Тег</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;VC2 VP2 VC3 VC…</td><td>1</td></tr><tr><td>&quot;VC3 H3 D&quot;</td><td>1</td></tr><tr><td>&quot;S1 H1 VC2&quot;</td><td>1</td></tr><tr><td>&quot;T1 T3&quot;</td><td>2</td></tr><tr><td>&quot;H3 S1 VC2&quot;</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;S1 VC1 VP1 VC2…</td><td>1</td></tr><tr><td>&quot;VC1 VC2 H2 VC4…</td><td>2</td></tr><tr><td>&quot;VC1 VP1 VP4 VC…</td><td>1</td></tr><tr><td>&quot;VP2 &quot;</td><td>7</td></tr><tr><td>&quot;S1 VC2 VP2&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_258, 2)\n",
       "┌──────────────────┬───────┐\n",
       "│ Тег              ┆ count │\n",
       "│ ---              ┆ ---   │\n",
       "│ str              ┆ u32   │\n",
       "╞══════════════════╪═══════╡\n",
       "│ VC2 VP2 VC3 VC4  ┆ 1     │\n",
       "│ VC3 H3 D         ┆ 1     │\n",
       "│ S1 H1 VC2        ┆ 1     │\n",
       "│ T1 T3            ┆ 2     │\n",
       "│ H3 S1 VC2        ┆ 1     │\n",
       "│ …                ┆ …     │\n",
       "│ S1 VC1 VP1 VC2   ┆ 1     │\n",
       "│ VC1 VC2 H2 VC4   ┆ 2     │\n",
       "│ VC1 VP1 VP4 VC4  ┆ 1     │\n",
       "│ VP2              ┆ 7     │\n",
       "│ S1 VC2 VP2       ┆ 9     │\n",
       "└──────────────────┴───────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Тег\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(\n",
    "    (pl.col(\"Тег\").apply(lambda x: \" \".join(re.findall(r\"[A-Z]{1,2}\\d|LMS\", x)))).alias(\"corrected_tag\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_filter = (\n",
    "    (pl.col(\"corrected_tag\").eq(\"\"))\n",
    ")\n",
    "\n",
    "dataset = dataset.filter(~null_filter)\n",
    "dataset = dataset.filter(~(pl.col(\"Комментарий\").is_null()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(\n",
    "    pl.col(\"corrected_tag\")\n",
    "    .str.replace_all(r\"VC4|VP4|VC5|S4|T4|H4|EA1\", \"\")\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s\\s+\", \" \")\n",
    "    .str.replace(r\"GH3\", \"H3\")\n",
    "    .str.replace(r\"HH3\", \"H3\")\n",
    "    .str.replace(r\"BP3\", \"VP3\")\n",
    "    .str.replace(r\"V3\", \"VC3\")\n",
    "    .str.replace(r\"V2\", \"VP2\")\n",
    ")\n",
    "\n",
    "dataset = dataset.filter(~(pl.col(\"corrected_tag\").eq(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (17, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>corrected_tag</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;H3&quot;</td><td>20391</td></tr><tr><td>&quot;VC2&quot;</td><td>14414</td></tr><tr><td>&quot;VC3&quot;</td><td>8111</td></tr><tr><td>&quot;VP3&quot;</td><td>4972</td></tr><tr><td>&quot;VP2&quot;</td><td>4897</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;LMS&quot;</td><td>669</td></tr><tr><td>&quot;T2&quot;</td><td>548</td></tr><tr><td>&quot;T1&quot;</td><td>399</td></tr><tr><td>&quot;T3&quot;</td><td>234</td></tr><tr><td>&quot;E2&quot;</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (17, 2)\n",
       "┌───────────────┬───────┐\n",
       "│ corrected_tag ┆ count │\n",
       "│ ---           ┆ ---   │\n",
       "│ str           ┆ u32   │\n",
       "╞═══════════════╪═══════╡\n",
       "│ H3            ┆ 20391 │\n",
       "│ VC2           ┆ 14414 │\n",
       "│ VC3           ┆ 8111  │\n",
       "│ VP3           ┆ 4972  │\n",
       "│ VP2           ┆ 4897  │\n",
       "│ …             ┆ …     │\n",
       "│ LMS           ┆ 669   │\n",
       "│ T2            ┆ 548   │\n",
       "│ T1            ┆ 399   │\n",
       "│ T3            ┆ 234   │\n",
       "│ E2            ┆ 1     │\n",
       "└───────────────┴───────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"corrected_tag\"].str.split(by = \" \").explode().value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(~pl.col(\"corrected_tag\").str.contains(\"E2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>corrected_tag</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;VC&quot;</td><td>26304</td></tr><tr><td>&quot;H&quot;</td><td>24437</td></tr><tr><td>&quot;VP&quot;</td><td>11560</td></tr><tr><td>&quot;S&quot;</td><td>1973</td></tr><tr><td>&quot;E&quot;</td><td>1785</td></tr><tr><td>&quot;T&quot;</td><td>1181</td></tr><tr><td>&quot;LMS&quot;</td><td>669</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 2)\n",
       "┌───────────────┬───────┐\n",
       "│ corrected_tag ┆ count │\n",
       "│ ---           ┆ ---   │\n",
       "│ str           ┆ u32   │\n",
       "╞═══════════════╪═══════╡\n",
       "│ VC            ┆ 26304 │\n",
       "│ H             ┆ 24437 │\n",
       "│ VP            ┆ 11560 │\n",
       "│ S             ┆ 1973  │\n",
       "│ E             ┆ 1785  │\n",
       "│ T             ┆ 1181  │\n",
       "│ LMS           ┆ 669   │\n",
       "└───────────────┴───────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_sub_tags(tags: str):\n",
    "    split = tags.split(sep=\" \")\n",
    "    new_tag = [x[:-1] if x[-1].isdigit() else x for x in split]\n",
    "    return \" \".join(new_tag)\n",
    "\n",
    "dataset = dataset.with_columns(\n",
    "    pl.col(\"corrected_tag\").apply(remove_sub_tags)\n",
    ")\n",
    "\n",
    "dataset[\"corrected_tag\"].str.split(by = \" \").explode().value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset[\"corrected_tag\"].str.split(by = \" \").explode().unique().sort().to_list()\n",
    "target = dict(zip(target, range(len(target))))\n",
    "reverse_target = {v : k for k, v in target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tags: str) -> list[float]:\n",
    "    \"\"\"Turn str with tags into list with digit labels.\n",
    "\n",
    "    Args:\n",
    "        tags (str): tag text representation.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: numeric labels.\n",
    "    \"\"\"\n",
    "    split = tags.split(sep = \" \")\n",
    "    res = np.zeros(len(target))\n",
    "    for x in split:\n",
    "        res[target[x]] = 1\n",
    "    return res.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(pl.col(\"corrected_tag\").apply(vectorize).alias(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (54_648, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Направление</th><th>Факультет</th><th>ID студента</th><th>Оценка</th><th>Категория</th><th>Тег</th><th>Комментарий</th><th>Статус</th><th>Neutral</th><th>Positive</th><th>Negative</th><th>Exclamations</th><th>have_code</th><th>Neutral_NLP</th><th>Positive_NLP</th><th>Negative_NLP</th><th>Speech_NLP</th><th>corrected_tag</th><th>labels</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;C&quot;</td><td>113.0</td><td>1493.0</td><td>1.0</td><td>&quot;Видео&quot;</td><td>&quot;VP2&quot;</td><td>&quot;Видео лагает&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>1.00001</td><td>0.010663</td><td>0.00001</td><td>0.00001</td><td>&quot;VP&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr><tr><td>&quot;C&quot;</td><td>113.0</td><td>5580.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Торгом Бабаян!…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5</td><td>false</td><td>0.437833</td><td>0.056662</td><td>0.140346</td><td>0.051855</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>126.0</td><td>5619.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3&quot;</td><td>&quot;Спасибо)&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.00001</td><td>0.00001</td><td>0.00001</td><td>1.00001</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>310.0</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2 E1&quot;</td><td>&quot;комментарий со…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.930468</td><td>0.025189</td><td>0.119213</td><td>0.000378</td><td>&quot;H E&quot;</td><td>[1.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>1913.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Жонибек, хочу …</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td><td>false</td><td>0.069552</td><td>0.217348</td><td>0.019134</td><td>0.507822</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Z&quot;</td><td>133.0</td><td>null</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2&quot;</td><td>&quot;требуемый форм…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.822199</td><td>0.013647</td><td>0.020974</td><td>0.00001</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;S1&quot;</td><td>&quot;заплатила и да…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.228166</td><td>0.042098</td><td>0.644235</td><td>0.006108</td><td>&quot;S&quot;</td><td>[0.0, 0.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>7.0</td><td>null</td><td>&quot;LMS&quot;</td><td>&quot;Крайне раздраж…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.692652</td><td>0.073706</td><td>0.262852</td><td>0.00523</td><td>&quot;LMS&quot;</td><td>[0.0, 0.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;VC2 VP2&quot;</td><td>&quot;Аналитик данны…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>true</td><td>0.705795</td><td>0.053413</td><td>0.320831</td><td>0.001511</td><td>&quot;VC VP&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;VP2 VC2&quot;</td><td>&quot;Системный анал…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.679189</td><td>0.092698</td><td>0.156115</td><td>0.00408</td><td>&quot;VP VC&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (54_648, 19)\n",
       "┌────────────┬───────────┬────────────┬────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ Направлени ┆ Факультет ┆ ID         ┆ Оценка ┆ … ┆ Negative_ ┆ Speech_NL ┆ corrected ┆ labels    │\n",
       "│ е          ┆ ---       ┆ студента   ┆ ---    ┆   ┆ NLP       ┆ P         ┆ _tag      ┆ ---       │\n",
       "│ ---        ┆ f64       ┆ ---        ┆ f64    ┆   ┆ ---       ┆ ---       ┆ ---       ┆ list[f64] │\n",
       "│ str        ┆           ┆ f64        ┆        ┆   ┆ f64       ┆ f64       ┆ str       ┆           │\n",
       "╞════════════╪═══════════╪════════════╪════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ C          ┆ 113.0     ┆ 1493.0     ┆ 1.0    ┆ … ┆ 0.00001   ┆ 0.00001   ┆ VP        ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "│ C          ┆ 113.0     ┆ 5580.0     ┆ 5.0    ┆ … ┆ 0.140346  ┆ 0.051855  ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 126.0     ┆ 5619.0     ┆ 5.0    ┆ … ┆ 0.00001   ┆ 1.00001   ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 123.0     ┆ 310.0      ┆ 3.0    ┆ … ┆ 0.119213  ┆ 0.000378  ┆ H E       ┆ [1.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 123.0     ┆ 1913.0     ┆ 5.0    ┆ … ┆ 0.019134  ┆ 0.507822  ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ …          ┆ …         ┆ …          ┆ …      ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ Z          ┆ 133.0     ┆ null       ┆ 3.0    ┆ … ┆ 0.020974  ┆ 0.00001   ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ 0.0    ┆ … ┆ 0.644235  ┆ 0.006108  ┆ S         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ 7.0    ┆ … ┆ 0.262852  ┆ 0.00523   ┆ LMS       ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ null   ┆ … ┆ 0.320831  ┆ 0.001511  ┆ VC VP     ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ null   ┆ … ┆ 0.156115  ┆ 0.00408   ┆ VP VC     ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "└────────────┴───────────┴────────────┴────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dataset = dataset.select(\n",
    "    pl.col(\"Комментарий\"),\n",
    "    pl.col(\"Направление\"),\n",
    "    pl.col(\"Факультет\"),\n",
    "    pl.col(\"Оценка\"),\n",
    "    pl.col(\"Neutral\"),\n",
    "    pl.col(\"Positive\"),\n",
    "    pl.col(\"Negative\"),\n",
    "    pl.col(\"Exclamations\"),\n",
    "    pl.col(\"have_code\"),\n",
    "    pl.col(\"Neutral_NLP\"),\n",
    "    pl.col(\"Positive_NLP\"),\n",
    "    pl.col(\"Negative_NLP\"),\n",
    "    pl.col(\"Speech_NLP\"),\n",
    "    pl.col(\"corrected_tag\"),\n",
    "    pl.col(\"labels\"),\n",
    "    pl.col(\"corrected_tag\").str.split(by=\" \").alias(\"temp\"),\n",
    ")\n",
    "clear_dataset = clear_dataset.explode(columns=[\"temp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    clear_dataset,\n",
    "    test_size=0.1,\n",
    "    random_state=3317,\n",
    "    stratify=clear_dataset[\"temp\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(columns=[\"corrected_tag\", \"temp\"])\n",
    "test_df = test_df.drop(columns=[\"corrected_tag\", \"temp\"])\n",
    "\n",
    "train_df = train_df.rename({\"Комментарий\": \"text\"})\n",
    "test_df = test_df.rename({\"Комментарий\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df.to_pandas(), split=\"train\")\n",
    "test_dataset = Dataset.from_pandas(test_df.to_pandas(), split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "def preprocess_data(sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Encode input text into sequence of tokens.\n",
    "    Also add corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        sample (dict[str, Any]): raw input text.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]: transformed sample with tokenized text and labels.\n",
    "    \"\"\"\n",
    "    text = sample[\"text\"]\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoding[\"labels\"] = sample[\"labels\"]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04a6bb5daf640b89fe5ef0174ac620f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f989e53200944ac8c8b4adbd1ae2c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6791 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train = train_dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "encoded_test = test_dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=test_dataset.column_names\n",
    ")\n",
    "encoded_train.set_format(\"torch\")\n",
    "encoded_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Training with FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, ignore_index=-100, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.CE = nn.CrossEntropyLoss(reduction='none', ignore_index=ignore_index)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        minus_logpt = self.CE(input, target)\n",
    "        pt = torch.exp(-minus_logpt) \n",
    "        focal_loss = (1-pt)**self.gamma * minus_logpt\n",
    "\n",
    "        if self.alpha != None:\n",
    "            focal_loss *= self.alpha.gather(0, target)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            focal_loss = focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            focal_loss = focal_loss.sum()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalBert(BertForSequenceClassification):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = nn.MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = MultiLabelFocalLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(\n",
    "    predictions: np.ndarray, labels: np.ndarray, threshold: float = 0.5\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute mltilabel metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.ndarray): logits array\n",
    "        labels (np.ndarray): labels array\n",
    "        threshold (float, optional): activation threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: metrics dict\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    f1_micro_average = f1_score(y_true=labels, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(labels, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"Metrics computation wrapper.\n",
    "\n",
    "    Args:\n",
    "        p (EvalPrediction): hf model output\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: metrics dict\n",
    "    \"\"\"\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_pipeline(\n",
    "    exp_name: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    train_dataset: Dataset,\n",
    "    eval_dataset: Dataset,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 2e-5,\n",
    "    epochs_num: int = 20,\n",
    "    model_name=None\n",
    ") -> Trainer:\n",
    "    \"\"\"Training process wrapper.\n",
    "\n",
    "    Args:\n",
    "        exp_name (str): name of the local folder\n",
    "        for saving model checkpoints.\n",
    "        tokenizer (AutoTokenizer): model tokenizer\n",
    "        train_dataset (Dataset): train dataset split\n",
    "        eval_dataset (Dataset): test dataset split\n",
    "        batch_size (int, optional): number of samples\n",
    "        in sigle batch. Defaults to 32.\n",
    "        lr (float, optional): model's learning rate. Defaults to 2e-5.\n",
    "        epochs_num (int, optional):\n",
    "        number of training iterations. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: hf training pipeline abstraction class.\n",
    "    \"\"\"\n",
    "    args = TrainingArguments(\n",
    "        exp_name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs_num,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    model = FocalBert.from_pretrained(\n",
    "        \"cointegrated/rubert-tiny2\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(target),\n",
    "        id2label=target,\n",
    "        label2id=reverse_target\n",
    "    )\n",
    "    if model_name is not None:\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 190\n",
    "EPOCHS = 75\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FocalBert were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = make_training_pipeline(\"f_focal\", tokenizer, encoded_train, encoded_test, batch_size=BATCH_SIZE, epochs_num=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9821' max='12075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9821/12075 1:34:08 < 21:36, 1.74 it/s, Epoch 61/75]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.352816</td>\n",
       "      <td>0.622865</td>\n",
       "      <td>0.742203</td>\n",
       "      <td>0.452511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276090</td>\n",
       "      <td>0.739378</td>\n",
       "      <td>0.821514</td>\n",
       "      <td>0.556766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.240829</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.851053</td>\n",
       "      <td>0.583566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.220402</td>\n",
       "      <td>0.795464</td>\n",
       "      <td>0.863187</td>\n",
       "      <td>0.612723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.869930</td>\n",
       "      <td>0.624503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.198192</td>\n",
       "      <td>0.810740</td>\n",
       "      <td>0.871455</td>\n",
       "      <td>0.635547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.190963</td>\n",
       "      <td>0.818527</td>\n",
       "      <td>0.877186</td>\n",
       "      <td>0.645119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.186070</td>\n",
       "      <td>0.822552</td>\n",
       "      <td>0.879243</td>\n",
       "      <td>0.652334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.181839</td>\n",
       "      <td>0.828879</td>\n",
       "      <td>0.885139</td>\n",
       "      <td>0.661464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.177369</td>\n",
       "      <td>0.831437</td>\n",
       "      <td>0.884065</td>\n",
       "      <td>0.666029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.173479</td>\n",
       "      <td>0.836499</td>\n",
       "      <td>0.890153</td>\n",
       "      <td>0.676336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.170658</td>\n",
       "      <td>0.839656</td>\n",
       "      <td>0.892404</td>\n",
       "      <td>0.681932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.168130</td>\n",
       "      <td>0.843487</td>\n",
       "      <td>0.895503</td>\n",
       "      <td>0.689442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.166281</td>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.894565</td>\n",
       "      <td>0.690914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.163295</td>\n",
       "      <td>0.848504</td>\n",
       "      <td>0.899402</td>\n",
       "      <td>0.700044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.161681</td>\n",
       "      <td>0.850189</td>\n",
       "      <td>0.900684</td>\n",
       "      <td>0.704904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.161188</td>\n",
       "      <td>0.851550</td>\n",
       "      <td>0.902461</td>\n",
       "      <td>0.708143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.159054</td>\n",
       "      <td>0.854485</td>\n",
       "      <td>0.904385</td>\n",
       "      <td>0.715359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.857385</td>\n",
       "      <td>0.907170</td>\n",
       "      <td>0.719923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.156927</td>\n",
       "      <td>0.860233</td>\n",
       "      <td>0.910085</td>\n",
       "      <td>0.724488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.156002</td>\n",
       "      <td>0.861699</td>\n",
       "      <td>0.911731</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.155641</td>\n",
       "      <td>0.863639</td>\n",
       "      <td>0.913588</td>\n",
       "      <td>0.734354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.155254</td>\n",
       "      <td>0.864222</td>\n",
       "      <td>0.913407</td>\n",
       "      <td>0.737888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.154124</td>\n",
       "      <td>0.866275</td>\n",
       "      <td>0.915368</td>\n",
       "      <td>0.741570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.153662</td>\n",
       "      <td>0.867179</td>\n",
       "      <td>0.917105</td>\n",
       "      <td>0.743779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.154380</td>\n",
       "      <td>0.867308</td>\n",
       "      <td>0.916896</td>\n",
       "      <td>0.745251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.154158</td>\n",
       "      <td>0.871765</td>\n",
       "      <td>0.920546</td>\n",
       "      <td>0.751583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.155141</td>\n",
       "      <td>0.871502</td>\n",
       "      <td>0.921810</td>\n",
       "      <td>0.752614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.155535</td>\n",
       "      <td>0.870767</td>\n",
       "      <td>0.919905</td>\n",
       "      <td>0.751583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>0.874753</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.760860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.155879</td>\n",
       "      <td>0.873127</td>\n",
       "      <td>0.922140</td>\n",
       "      <td>0.757768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.154741</td>\n",
       "      <td>0.874759</td>\n",
       "      <td>0.922719</td>\n",
       "      <td>0.762332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.154326</td>\n",
       "      <td>0.875877</td>\n",
       "      <td>0.924351</td>\n",
       "      <td>0.763069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.154277</td>\n",
       "      <td>0.876980</td>\n",
       "      <td>0.925202</td>\n",
       "      <td>0.765425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.155720</td>\n",
       "      <td>0.876250</td>\n",
       "      <td>0.925487</td>\n",
       "      <td>0.763511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.156120</td>\n",
       "      <td>0.876046</td>\n",
       "      <td>0.924365</td>\n",
       "      <td>0.766897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.155411</td>\n",
       "      <td>0.877889</td>\n",
       "      <td>0.926352</td>\n",
       "      <td>0.770873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.154458</td>\n",
       "      <td>0.879738</td>\n",
       "      <td>0.926566</td>\n",
       "      <td>0.773229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.156452</td>\n",
       "      <td>0.879171</td>\n",
       "      <td>0.927255</td>\n",
       "      <td>0.772640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.156044</td>\n",
       "      <td>0.880133</td>\n",
       "      <td>0.927284</td>\n",
       "      <td>0.774555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.156788</td>\n",
       "      <td>0.879470</td>\n",
       "      <td>0.927803</td>\n",
       "      <td>0.772935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.157993</td>\n",
       "      <td>0.880584</td>\n",
       "      <td>0.928445</td>\n",
       "      <td>0.776469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.156933</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.928526</td>\n",
       "      <td>0.777352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.156568</td>\n",
       "      <td>0.882059</td>\n",
       "      <td>0.927980</td>\n",
       "      <td>0.782506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.157897</td>\n",
       "      <td>0.882572</td>\n",
       "      <td>0.929624</td>\n",
       "      <td>0.780445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.157248</td>\n",
       "      <td>0.881644</td>\n",
       "      <td>0.929178</td>\n",
       "      <td>0.779414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.157179</td>\n",
       "      <td>0.883243</td>\n",
       "      <td>0.929639</td>\n",
       "      <td>0.783979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.157824</td>\n",
       "      <td>0.882642</td>\n",
       "      <td>0.929716</td>\n",
       "      <td>0.783095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.159655</td>\n",
       "      <td>0.882784</td>\n",
       "      <td>0.931111</td>\n",
       "      <td>0.780445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.159448</td>\n",
       "      <td>0.883699</td>\n",
       "      <td>0.930983</td>\n",
       "      <td>0.783390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.159588</td>\n",
       "      <td>0.882970</td>\n",
       "      <td>0.930238</td>\n",
       "      <td>0.784126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.159096</td>\n",
       "      <td>0.883630</td>\n",
       "      <td>0.930892</td>\n",
       "      <td>0.785157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.160153</td>\n",
       "      <td>0.884129</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.785010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.161098</td>\n",
       "      <td>0.884788</td>\n",
       "      <td>0.932368</td>\n",
       "      <td>0.785893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.161001</td>\n",
       "      <td>0.884318</td>\n",
       "      <td>0.932054</td>\n",
       "      <td>0.785304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.161799</td>\n",
       "      <td>0.884820</td>\n",
       "      <td>0.932199</td>\n",
       "      <td>0.787218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.161493</td>\n",
       "      <td>0.885928</td>\n",
       "      <td>0.932554</td>\n",
       "      <td>0.788691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.161149</td>\n",
       "      <td>0.885983</td>\n",
       "      <td>0.932320</td>\n",
       "      <td>0.790458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>0.885434</td>\n",
       "      <td>0.932162</td>\n",
       "      <td>0.789280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.161762</td>\n",
       "      <td>0.885655</td>\n",
       "      <td>0.932618</td>\n",
       "      <td>0.789280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.885577</td>\n",
       "      <td>0.932775</td>\n",
       "      <td>0.788396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9821, training_loss=0.10332825630311507, metrics={'train_runtime': 5649.8407, 'train_samples_per_second': 811.324, 'train_steps_per_second': 2.137, 'total_flos': 2.751042768246989e+16, 'train_loss': 0.10332825630311507, 'epoch': 61.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16114938259124756,\n",
       " 'eval_f1': 0.8859830473433946,\n",
       " 'eval_roc_auc': 0.93231951687974,\n",
       " 'eval_accuracy': 0.7904579590634664,\n",
       " 'eval_runtime': 4.2563,\n",
       " 'eval_samples_per_second': 1595.508,\n",
       " 'eval_steps_per_second': 4.229,\n",
       " 'epoch': 61.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = trainer.predict(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.8859830473433946, 'roc_auc': 0.93231951687974, 'accuracy': 0.7904579590634664}\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = make_training_pipeline(\"v\", tokenizer, encoded_train, encoded_test, batch_size=BATCH_SIZE, epochs_num=EPOCHS, lr=LR, model_name=\"f/checkpoint-4590/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_preds = trainer.predict(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать сравнение с Focal Loss и без него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'Направление', 'Факультет', 'Оценка', 'Neutral', 'Positive', 'Negative', 'Exclamations', 'have_code', 'Neutral_NLP', 'Positive_NLP', 'Negative_NLP', 'Speech_NLP', 'labels'],\n",
       "    num_rows: 61118\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = pd.get_dummies(train_df.to_pandas()[\"Направление\"])\n",
    "departments = pd.get_dummies(train_df.to_pandas()[\"Факультет\"])\n",
    "meta_dataset_train = train_df.select(pl.exclude(\"Направление\", \"Факультет\")).to_pandas()\n",
    "meta_dataset_train = pd.concat([meta_dataset_train, directions, departments, pd.DataFrame(train_preds.predictions)], axis=1)\n",
    "meta_dataset_train = meta_dataset_train.drop(columns=[\"text\"])\n",
    "\n",
    "meta_dataset_test = test_df.select(pl.exclude(\"Направление\", \"Факультет\")).to_pandas()\n",
    "meta_dataset_test = pd.concat([meta_dataset_test, directions, departments, pd.DataFrame(test_preds.predictions)], axis=1)\n",
    "meta_dataset_test = meta_dataset_test.drop(columns=[\"text\"])\n",
    "\n",
    "meta_dataset_test = meta_dataset_test.dropna(subset=[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = meta_dataset_train.drop('labels', axis=1), np.array(meta_dataset_train[\"labels\"].to_list())\n",
    "X_test, y_test = meta_dataset_test.drop('labels', axis=1), np.array(meta_dataset_test[\"labels\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=trial.suggest_int(\"iterations\", 500, 2000),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0, log=True),\n",
    "        bootstrap_type=trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\"]),\n",
    "        random_strength=trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "        od_type=trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        od_wait=trial.suggest_int(\"od_wait\", 10, 50),\n",
    "        verbose=False,\n",
    "        task_type=\"GPU\",\n",
    "        devices='0',\n",
    "        loss_function = trial.suggest_categorical(\"loss_function\", [\"MultiCrossEntropy\", \"MultiLogloss\"]))\n",
    "    model.fit(train_pool, eval_set=test_pool)\n",
    "    y_pred = model.predict(test_pool)\n",
    "    return hamming_loss(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-23 00:20:46,405] A new study created in memory with name: catboost\n",
      "Warning: less than 75% gpu memory available for training. Free: 1158.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:20:52,849] Trial 0 finished with value: 0.04768916843721733 and parameters: {'iterations': 893, 'learning_rate': 0.0020766721769608226, 'depth': 5, 'l2_leaf_reg': 0.00039189423261207457, 'bootstrap_type': 'Bayesian', 'random_strength': 7.74470472863429e-06, 'bagging_temperature': 5.183928205975371, 'od_type': 'Iter', 'od_wait': 40, 'loss_function': 'MultiLogloss'}. Best is trial 0 with value: 0.04768916843721733.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:21:25,381] Trial 1 finished with value: 0.04653217493741717 and parameters: {'iterations': 1443, 'learning_rate': 0.0017787538482591226, 'depth': 10, 'l2_leaf_reg': 0.00027055071527787685, 'bootstrap_type': 'Bayesian', 'random_strength': 0.12765125185466167, 'bagging_temperature': 7.941185757915866, 'od_type': 'Iter', 'od_wait': 33, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04653217493741717.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:21:26,498] Trial 2 finished with value: 0.046658392410122644 and parameters: {'iterations': 932, 'learning_rate': 0.021899677346699262, 'depth': 7, 'l2_leaf_reg': 6.105015510183205e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 5.227231242013996e-05, 'bagging_temperature': 1.991952352050984, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04653217493741717.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:21:28,534] Trial 3 finished with value: 0.047478805982708205 and parameters: {'iterations': 1256, 'learning_rate': 0.010494020147150274, 'depth': 4, 'l2_leaf_reg': 0.20744390859184064, 'bootstrap_type': 'Bayesian', 'random_strength': 8.704530653536676, 'bagging_temperature': 1.6287753326976728, 'od_type': 'Iter', 'od_wait': 38, 'loss_function': 'MultiLogloss'}. Best is trial 1 with value: 0.04653217493741717.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:21:35,977] Trial 4 finished with value: 0.046490102446515344 and parameters: {'iterations': 588, 'learning_rate': 0.03790287646860259, 'depth': 6, 'l2_leaf_reg': 0.10598324121909684, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0002636705926743837, 'bagging_temperature': 9.728723298688061, 'od_type': 'IncToDec', 'od_wait': 33, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:22:15,113] Trial 5 finished with value: 0.04726844352819909 and parameters: {'iterations': 521, 'learning_rate': 0.0015069197830871604, 'depth': 10, 'l2_leaf_reg': 54.274270357483914, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0002650536373654029, 'bagging_temperature': 3.409359716532486, 'od_type': 'IncToDec', 'od_wait': 41, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:22:15,666] Trial 6 finished with value: 0.047205334791846354 and parameters: {'iterations': 1500, 'learning_rate': 0.06641173145737062, 'depth': 5, 'l2_leaf_reg': 12.758016626212465, 'bootstrap_type': 'Bayesian', 'random_strength': 8.302734146614306e-06, 'bagging_temperature': 7.00298420049446, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:22:34,178] Trial 7 finished with value: 0.0467635736373772 and parameters: {'iterations': 1094, 'learning_rate': 0.03835755070376457, 'depth': 7, 'l2_leaf_reg': 0.1735717250993288, 'bootstrap_type': 'Bayesian', 'random_strength': 0.13715362597057945, 'bagging_temperature': 3.5152409979850177, 'od_type': 'IncToDec', 'od_wait': 46, 'loss_function': 'MultiLogloss'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:23:01,130] Trial 8 finished with value: 0.04672150114647538 and parameters: {'iterations': 1856, 'learning_rate': 0.01849682603070215, 'depth': 7, 'l2_leaf_reg': 2.3719615831810452e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 3.30016207773554, 'bagging_temperature': 5.594301234747533, 'od_type': 'IncToDec', 'od_wait': 27, 'loss_function': 'MultiLogloss'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:23:06,684] Trial 9 finished with value: 0.0476470959463155 and parameters: {'iterations': 688, 'learning_rate': 0.023297333198997773, 'depth': 4, 'l2_leaf_reg': 2.1798631044417786e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 0.04245980204881909, 'bagging_temperature': 9.342098037151883, 'od_type': 'IncToDec', 'od_wait': 45, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:24:24,949] Trial 10 finished with value: 0.04699497233733723 and parameters: {'iterations': 1990, 'learning_rate': 0.005176055451095398, 'depth': 9, 'l2_leaf_reg': 0.04959569101999484, 'bootstrap_type': 'Bayesian', 'random_strength': 1.3994412402008133e-08, 'bagging_temperature': 0.2102342761106204, 'od_type': 'IncToDec', 'od_wait': 26, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:24:31,396] Trial 11 finished with value: 0.04663735616467173 and parameters: {'iterations': 1518, 'learning_rate': 0.004058893763673794, 'depth': 8, 'l2_leaf_reg': 0.00018647191909609465, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0045692725527288975, 'bagging_temperature': 9.263117545158487, 'od_type': 'Iter', 'od_wait': 33, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:24:48,898] Trial 12 finished with value: 0.047226371037297266 and parameters: {'iterations': 1448, 'learning_rate': 0.0010141257391981069, 'depth': 6, 'l2_leaf_reg': 0.0032887753708398465, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0032653465794882895, 'bagging_temperature': 7.647287200469777, 'od_type': 'IncToDec', 'od_wait': 32, 'loss_function': 'MultiCrossEntropy'}. Best is trial 4 with value: 0.046490102446515344.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:24:50,688] Trial 13 finished with value: 0.04609041378294802 and parameters: {'iterations': 1699, 'learning_rate': 0.08949523912226792, 'depth': 10, 'l2_leaf_reg': 7.674731241803363e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 1.8084451393310196e-07, 'bagging_temperature': 7.710292433867646, 'od_type': 'Iter', 'od_wait': 23, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:24:51,202] Trial 14 finished with value: 0.04707911731914088 and parameters: {'iterations': 1761, 'learning_rate': 0.09762467235254711, 'depth': 8, 'l2_leaf_reg': 2.4885261171471216e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 5.688353217329838e-08, 'bagging_temperature': 9.965088440434045, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:11,439] Trial 15 finished with value: 0.04701600858278814 and parameters: {'iterations': 1738, 'learning_rate': 0.05006487621020804, 'depth': 6, 'l2_leaf_reg': 3.586147326979207e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 2.649177883325444e-07, 'bagging_temperature': 6.882255755704362, 'od_type': 'IncToDec', 'od_wait': 24, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:12,724] Trial 16 finished with value: 0.046363884973809874 and parameters: {'iterations': 1230, 'learning_rate': 0.08234080921678685, 'depth': 9, 'l2_leaf_reg': 0.008545108122281939, 'bootstrap_type': 'Bayesian', 'random_strength': 9.325828854951632e-07, 'bagging_temperature': 8.67341499519859, 'od_type': 'Iter', 'od_wait': 21, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:13,812] Trial 17 finished with value: 0.04716326230094453 and parameters: {'iterations': 1283, 'learning_rate': 0.09875266773534647, 'depth': 9, 'l2_leaf_reg': 0.006592814504838736, 'bootstrap_type': 'Bayesian', 'random_strength': 6.019373903471912e-07, 'bagging_temperature': 8.395985903537378, 'od_type': 'Iter', 'od_wait': 20, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:18,552] Trial 18 finished with value: 0.04714222605549361 and parameters: {'iterations': 1073, 'learning_rate': 0.009855647008607518, 'depth': 9, 'l2_leaf_reg': 1.5966042002040544, 'bootstrap_type': 'Bayesian', 'random_strength': 1.4750152064836782e-06, 'bagging_temperature': 6.665706596933646, 'od_type': 'Iter', 'od_wait': 14, 'loss_function': 'MultiLogloss'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:20,651] Trial 19 finished with value: 0.04646906620106443 and parameters: {'iterations': 1607, 'learning_rate': 0.06199094924291737, 'depth': 10, 'l2_leaf_reg': 1.642284441465017e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 9.836225466238039e-08, 'bagging_temperature': 8.40571947128251, 'od_type': 'Iter', 'od_wait': 21, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:22,093] Trial 20 finished with value: 0.046910827355533584 and parameters: {'iterations': 1323, 'learning_rate': 0.03279462850952623, 'depth': 8, 'l2_leaf_reg': 0.008693747049246815, 'bootstrap_type': 'Bayesian', 'random_strength': 6.25099088549019e-06, 'bagging_temperature': 6.094610610367813, 'od_type': 'Iter', 'od_wait': 28, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:24,288] Trial 21 finished with value: 0.046931863600984496 and parameters: {'iterations': 1640, 'learning_rate': 0.0626424709275884, 'depth': 10, 'l2_leaf_reg': 1.9222418158535277e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 4.948991067434057e-08, 'bagging_temperature': 8.428424213306014, 'od_type': 'Iter', 'od_wait': 22, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:26,298] Trial 22 finished with value: 0.046384921219260786 and parameters: {'iterations': 1596, 'learning_rate': 0.0723705979338432, 'depth': 10, 'l2_leaf_reg': 2.6723284063710703e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 1.0454709704489723e-08, 'bagging_temperature': 8.826009486987315, 'od_type': 'Iter', 'od_wait': 20, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:27,310] Trial 23 finished with value: 0.04617455876475167 and parameters: {'iterations': 1944, 'learning_rate': 0.09653960010333662, 'depth': 9, 'l2_leaf_reg': 1.0210066855750554e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 1.0468172984617207e-08, 'bagging_temperature': 8.879755160012396, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:28,215] Trial 24 finished with value: 0.04621663125565349 and parameters: {'iterations': 1993, 'learning_rate': 0.09739262149903259, 'depth': 9, 'l2_leaf_reg': 1.7745587217380487e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 1.2711669311558763e-06, 'bagging_temperature': 7.423246467136714, 'od_type': 'Iter', 'od_wait': 12, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:29,365] Trial 25 finished with value: 0.0478153859099228 and parameters: {'iterations': 1966, 'learning_rate': 0.049718745566752265, 'depth': 9, 'l2_leaf_reg': 1.3180904884414677e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 1.4218217579094861e-07, 'bagging_temperature': 7.225369945368858, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:31,401] Trial 26 finished with value: 0.04646906620106443 and parameters: {'iterations': 1856, 'learning_rate': 0.01328436615300951, 'depth': 8, 'l2_leaf_reg': 8.573616822611584e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 2.6198401639456187e-06, 'bagging_temperature': 4.285902259573559, 'od_type': 'Iter', 'od_wait': 14, 'loss_function': 'MultiLogloss'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:32,972] Trial 27 finished with value: 0.04632181248290805 and parameters: {'iterations': 1861, 'learning_rate': 0.03327570654785276, 'depth': 9, 'l2_leaf_reg': 2.2466932779759058e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 3.311146470346169e-05, 'bagging_temperature': 6.244109912439338, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:34,902] Trial 28 finished with value: 0.04699497233733723 and parameters: {'iterations': 1751, 'learning_rate': 0.05146319155647162, 'depth': 10, 'l2_leaf_reg': 1.3455506674538773e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 2.9217103832157294e-08, 'bagging_temperature': 7.632175449675621, 'od_type': 'Iter', 'od_wait': 13, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:38,452] Trial 29 finished with value: 0.04684771861918085 and parameters: {'iterations': 1919, 'learning_rate': 0.00689396229210286, 'depth': 8, 'l2_leaf_reg': 9.998763012821783e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 3.194640259304357e-07, 'bagging_temperature': 4.889122963483224, 'od_type': 'Iter', 'od_wait': 12, 'loss_function': 'MultiLogloss'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:48,741] Trial 30 finished with value: 0.046679428655573556 and parameters: {'iterations': 2000, 'learning_rate': 0.0031687537513670986, 'depth': 9, 'l2_leaf_reg': 1.1721811208644664e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 1.9078936251285544e-05, 'bagging_temperature': 5.526415669389329, 'od_type': 'Iter', 'od_wait': 16, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:49,798] Trial 31 finished with value: 0.04646906620106443 and parameters: {'iterations': 1843, 'learning_rate': 0.09841148853578273, 'depth': 9, 'l2_leaf_reg': 3.046609175229982e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 6.985043537662336e-05, 'bagging_temperature': 7.7831194535301815, 'od_type': 'Iter', 'od_wait': 18, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:53,081] Trial 32 finished with value: 0.046384921219260786 and parameters: {'iterations': 1706, 'learning_rate': 0.027203528282203848, 'depth': 10, 'l2_leaf_reg': 1.5521825478752904e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 2.326751708941059e-06, 'bagging_temperature': 6.345992924299488, 'od_type': 'Iter', 'od_wait': 23, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:25:54,526] Trial 33 finished with value: 0.04657424742831899 and parameters: {'iterations': 1876, 'learning_rate': 0.04475024755397741, 'depth': 9, 'l2_leaf_reg': 3.122981056112334e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 3.563875672357161e-05, 'bagging_temperature': 6.1035881800820615, 'od_type': 'Iter', 'od_wait': 16, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:00,545] Trial 34 finished with value: 0.04619559501020258 and parameters: {'iterations': 1791, 'learning_rate': 0.016429694968297305, 'depth': 10, 'l2_leaf_reg': 0.00020179107158513072, 'bootstrap_type': 'Bayesian', 'random_strength': 9.627160222136996e-06, 'bagging_temperature': 7.385172389419591, 'od_type': 'Iter', 'od_wait': 50, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:06,605] Trial 35 finished with value: 0.04707911731914088 and parameters: {'iterations': 1671, 'learning_rate': 0.01712819284390229, 'depth': 10, 'l2_leaf_reg': 0.000488299031606337, 'bootstrap_type': 'Bayesian', 'random_strength': 2.839050758762992e-07, 'bagging_temperature': 8.02060839086076, 'od_type': 'Iter', 'od_wait': 50, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:09,032] Trial 36 finished with value: 0.0464059574647117 and parameters: {'iterations': 1807, 'learning_rate': 0.08094711300381394, 'depth': 10, 'l2_leaf_reg': 0.0014089600221019495, 'bootstrap_type': 'Bayesian', 'random_strength': 7.022519520221378e-06, 'bagging_temperature': 9.180804526609275, 'od_type': 'Iter', 'od_wait': 30, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:18,137] Trial 37 finished with value: 0.04634284872835896 and parameters: {'iterations': 1932, 'learning_rate': 0.007789578165433368, 'depth': 10, 'l2_leaf_reg': 7.942449829729957e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 3.8920119739773736e-08, 'bagging_temperature': 7.391924348959933, 'od_type': 'Iter', 'od_wait': 36, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:18,881] Trial 38 finished with value: 0.046658392410122644 and parameters: {'iterations': 1540, 'learning_rate': 0.06533431679239703, 'depth': 7, 'l2_leaf_reg': 4.963264726187313e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00010846905559401879, 'bagging_temperature': 8.099729090789664, 'od_type': 'Iter', 'od_wait': 25, 'loss_function': 'MultiLogloss'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:41,880] Trial 39 finished with value: 0.046658392410122644 and parameters: {'iterations': 1390, 'learning_rate': 0.0024971049156015616, 'depth': 10, 'l2_leaf_reg': 0.00015860532537573593, 'bootstrap_type': 'Bayesian', 'random_strength': 5.857139726787521e-07, 'bagging_temperature': 4.923582890229754, 'od_type': 'Iter', 'od_wait': 41, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:43,570] Trial 40 finished with value: 0.04699497233733723 and parameters: {'iterations': 1924, 'learning_rate': 0.025843749752153772, 'depth': 8, 'l2_leaf_reg': 4.535971367528302e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0012158141288501334, 'bagging_temperature': 9.596131275911164, 'od_type': 'Iter', 'od_wait': 37, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:45,299] Trial 41 finished with value: 0.0465952836737699 and parameters: {'iterations': 1799, 'learning_rate': 0.03401542137175697, 'depth': 9, 'l2_leaf_reg': 6.37991412631505e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 1.1723357902808928e-05, 'bagging_temperature': 6.541994958769928, 'od_type': 'Iter', 'od_wait': 18, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:48,240] Trial 42 finished with value: 0.0467635736373772 and parameters: {'iterations': 1888, 'learning_rate': 0.014815048955393742, 'depth': 9, 'l2_leaf_reg': 3.066400920582772e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0004812117688665928, 'bagging_temperature': 7.141976923509301, 'od_type': 'Iter', 'od_wait': 15, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:50,672] Trial 43 finished with value: 0.04661631991922082 and parameters: {'iterations': 1998, 'learning_rate': 0.019924485836396544, 'depth': 9, 'l2_leaf_reg': 3.03053806737096e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 3.7155039450420824e-06, 'bagging_temperature': 5.51303939099823, 'od_type': 'Iter', 'od_wait': 19, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:26:53,193] Trial 44 finished with value: 0.046658392410122644 and parameters: {'iterations': 823, 'learning_rate': 0.03805367382265465, 'depth': 10, 'l2_leaf_reg': 0.030296131849993184, 'bootstrap_type': 'Bayesian', 'random_strength': 3.1759141551000574e-05, 'bagging_temperature': 5.947610990965645, 'od_type': 'Iter', 'od_wait': 12, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:28:40,864] Trial 45 finished with value: 0.0464059574647117 and parameters: {'iterations': 1683, 'learning_rate': 0.02903168533000524, 'depth': 10, 'l2_leaf_reg': 7.12001980110031e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0001579421293860931, 'bagging_temperature': 4.321518943963815, 'od_type': 'IncToDec', 'od_wait': 28, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:28:42,178] Trial 46 finished with value: 0.04663735616467173 and parameters: {'iterations': 1793, 'learning_rate': 0.07703730814767348, 'depth': 8, 'l2_leaf_reg': 1.1223420189132371e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 1.2701678129997156e-07, 'bagging_temperature': 9.010318699134297, 'od_type': 'Iter', 'od_wait': 46, 'loss_function': 'MultiLogloss'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:29:41,180] Trial 47 finished with value: 0.046658392410122644 and parameters: {'iterations': 1567, 'learning_rate': 0.05629907159651271, 'depth': 9, 'l2_leaf_reg': 1.2392124146988884e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 2.0544114984118048e-08, 'bagging_temperature': 2.113091351198677, 'od_type': 'IncToDec', 'od_wait': 50, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:29:42,321] Trial 48 finished with value: 0.04672150114647538 and parameters: {'iterations': 1730, 'learning_rate': 0.04057010825568509, 'depth': 7, 'l2_leaf_reg': 6.442080070439631e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 9.043288891930899e-07, 'bagging_temperature': 6.892908149210182, 'od_type': 'Iter', 'od_wait': 44, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n",
      "Warning: less than 75% gpu memory available for training. Free: 1156.1875 Total: 32494.125\n",
      "[I 2024-05-23 00:29:45,689] Trial 49 finished with value: 0.04657424742831899 and parameters: {'iterations': 1467, 'learning_rate': 0.012933183797189971, 'depth': 9, 'l2_leaf_reg': 0.0011185983025082079, 'bootstrap_type': 'Bayesian', 'random_strength': 1.3342243795733601e-05, 'bagging_temperature': 7.441106575621463, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 13 with value: 0.04609041378294802.\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=1337)\n",
    "study = optuna.create_study(study_name=\"catboost\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value:  0.04609041378294802\n",
      "  Params: \n",
      "    iterations: 1699\n",
      "    learning_rate: 0.08949523912226792\n",
      "    depth: 10\n",
      "    l2_leaf_reg: 7.674731241803363e-05\n",
      "    bootstrap_type: Bayesian\n",
      "    random_strength: 1.8084451393310196e-07\n",
      "    bagging_temperature: 7.710292433867646\n",
      "    od_type: Iter\n",
      "    od_wait: 23\n",
      "    loss_function: MultiCrossEntropy\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4319112\ttest: 0.4491328\tbest: 0.4491328 (0)\ttotal: 228ms\tremaining: 6m 26s\n",
      "1:\tlearn: 0.2655963\ttest: 0.3014698\tbest: 0.3014698 (1)\ttotal: 464ms\tremaining: 6m 33s\n",
      "2:\tlearn: 0.1758874\ttest: 0.2264591\tbest: 0.2264591 (2)\ttotal: 697ms\tremaining: 6m 33s\n",
      "3:\tlearn: 0.1199314\ttest: 0.1836601\tbest: 0.1836601 (3)\ttotal: 920ms\tremaining: 6m 29s\n",
      "4:\tlearn: 0.0883319\ttest: 0.1635228\tbest: 0.1635228 (4)\ttotal: 1.16s\tremaining: 6m 32s\n",
      "5:\tlearn: 0.0701446\ttest: 0.1556353\tbest: 0.1556353 (5)\ttotal: 1.4s\tremaining: 6m 33s\n",
      "6:\tlearn: 0.0568605\ttest: 0.1515256\tbest: 0.1515256 (6)\ttotal: 1.62s\tremaining: 6m 30s\n",
      "7:\tlearn: 0.0481014\ttest: 0.1515955\tbest: 0.1515256 (6)\ttotal: 1.85s\tremaining: 6m 30s\n",
      "8:\tlearn: 0.0426349\ttest: 0.1527650\tbest: 0.1515256 (6)\ttotal: 2.08s\tremaining: 6m 31s\n",
      "9:\tlearn: 0.0401586\ttest: 0.1538804\tbest: 0.1515256 (6)\ttotal: 2.24s\tremaining: 6m 18s\n",
      "10:\tlearn: 0.0379284\ttest: 0.1551610\tbest: 0.1515256 (6)\ttotal: 2.38s\tremaining: 6m 4s\n",
      "11:\tlearn: 0.0363394\ttest: 0.1562246\tbest: 0.1515256 (6)\ttotal: 2.51s\tremaining: 5m 53s\n",
      "12:\tlearn: 0.0349347\ttest: 0.1580670\tbest: 0.1515256 (6)\ttotal: 2.68s\tremaining: 5m 47s\n",
      "13:\tlearn: 0.0335662\ttest: 0.1599560\tbest: 0.1515256 (6)\ttotal: 2.83s\tremaining: 5m 40s\n",
      "14:\tlearn: 0.0317430\ttest: 0.1633786\tbest: 0.1515256 (6)\ttotal: 3.08s\tremaining: 5m 45s\n",
      "15:\tlearn: 0.0305596\ttest: 0.1653649\tbest: 0.1515256 (6)\ttotal: 3.33s\tremaining: 5m 50s\n",
      "16:\tlearn: 0.0297492\ttest: 0.1668277\tbest: 0.1515256 (6)\ttotal: 3.46s\tremaining: 5m 42s\n",
      "17:\tlearn: 0.0293273\ttest: 0.1677308\tbest: 0.1515256 (6)\ttotal: 3.6s\tremaining: 5m 35s\n",
      "18:\tlearn: 0.0288928\ttest: 0.1684309\tbest: 0.1515256 (6)\ttotal: 3.74s\tremaining: 5m 30s\n",
      "19:\tlearn: 0.0284946\ttest: 0.1693415\tbest: 0.1515256 (6)\ttotal: 3.87s\tremaining: 5m 25s\n",
      "20:\tlearn: 0.0281762\ttest: 0.1703208\tbest: 0.1515256 (6)\ttotal: 4.02s\tremaining: 5m 21s\n",
      "21:\tlearn: 0.0275426\ttest: 0.1710587\tbest: 0.1515256 (6)\ttotal: 4.26s\tremaining: 5m 25s\n",
      "22:\tlearn: 0.0270847\ttest: 0.1720542\tbest: 0.1515256 (6)\ttotal: 4.41s\tremaining: 5m 21s\n",
      "23:\tlearn: 0.0266380\ttest: 0.1731752\tbest: 0.1515256 (6)\ttotal: 4.58s\tremaining: 5m 19s\n",
      "24:\tlearn: 0.0264316\ttest: 0.1736337\tbest: 0.1515256 (6)\ttotal: 4.71s\tremaining: 5m 15s\n",
      "25:\tlearn: 0.0260886\ttest: 0.1745023\tbest: 0.1515256 (6)\ttotal: 4.83s\tremaining: 5m 10s\n",
      "26:\tlearn: 0.0256615\ttest: 0.1760207\tbest: 0.1515256 (6)\ttotal: 5.07s\tremaining: 5m 13s\n",
      "27:\tlearn: 0.0253828\ttest: 0.1769436\tbest: 0.1515256 (6)\ttotal: 5.21s\tremaining: 5m 10s\n",
      "28:\tlearn: 0.0249659\ttest: 0.1786374\tbest: 0.1515256 (6)\ttotal: 5.44s\tremaining: 5m 13s\n",
      "29:\tlearn: 0.0246341\ttest: 0.1798307\tbest: 0.1515256 (6)\ttotal: 5.59s\tremaining: 5m 11s\n",
      "Stopped by overfitting detector  (23 iterations wait)\n",
      "\n",
      "bestTest = 0.1515256209\n",
      "bestIteration = 6\n",
      "\n",
      "Shrink model to first 7 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(**trial.params, verbose=True)\n",
    "model.fit(train_pool, eval_set=test_pool)\n",
    "pred_labels = model.predict(test_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score  support\n",
      "0              0.738739  0.747720  0.743202    329.0\n",
      "1              0.875122  0.922523  0.898198   2917.0\n",
      "2              0.894737  0.531250  0.666667    128.0\n",
      "3              0.862319  0.860241  0.861279    415.0\n",
      "4              0.731092  0.595890  0.656604    146.0\n",
      "5              0.883407  0.914762  0.898811   3719.0\n",
      "6              0.882719  0.893382  0.888019   1904.0\n",
      "micro avg      0.873099  0.894748  0.883791   9558.0\n",
      "macro avg      0.838305  0.780824  0.801826   9558.0\n",
      "weighted avg   0.872671  0.894748  0.882679   9558.0\n",
      "samples avg    0.857532  0.875853  0.858406   9558.0\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(np.array(meta_dataset_test[\"labels\"].to_list()), pred_labels, output_dict=True)\n",
    "cr = pd.DataFrame(cr).T\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
