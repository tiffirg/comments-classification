{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import seaborn as sns\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "from datasets import load_dataset, Dataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation as PUNCT\n",
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    multilabel_confusion_matrix,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    hamming_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Направление</th><th>Факультет</th><th>ID студента</th><th>Оценка</th><th>Категория</th><th>Тег</th><th>Комментарий</th><th>Статус</th><th>Neutral</th><th>Positive</th><th>Negative</th><th>Exclamations</th><th>have_code</th><th>Neutral_NLP</th><th>Positive_NLP</th><th>Negative_NLP</th><th>Speech_NLP</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;C&quot;</td><td>113.0</td><td>1493.0</td><td>1.0</td><td>&quot;Видео&quot;</td><td>&quot;VP2&quot;</td><td>&quot;Видео лагает&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>1.00001</td><td>0.010663</td><td>0.00001</td><td>0.00001</td></tr><tr><td>&quot;C&quot;</td><td>113.0</td><td>5580.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Торгом Бабаян!…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5</td><td>false</td><td>0.437833</td><td>0.056662</td><td>0.140346</td><td>0.051855</td></tr><tr><td>&quot;E&quot;</td><td>126.0</td><td>5619.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3&quot;</td><td>&quot;Спасибо)&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.00001</td><td>0.00001</td><td>0.00001</td><td>1.00001</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>310.0</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2 E1&quot;</td><td>&quot;комментарий со…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.930468</td><td>0.025189</td><td>0.119213</td><td>0.000378</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>1913.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Жонибек, хочу …</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td><td>false</td><td>0.069552</td><td>0.217348</td><td>0.019134</td><td>0.507822</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 17)\n",
       "┌────────────┬───────────┬────────────┬────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ Направлени ┆ Факультет ┆ ID         ┆ Оценка ┆ … ┆ Neutral_N ┆ Positive_ ┆ Negative_ ┆ Speech_NL │\n",
       "│ е          ┆ ---       ┆ студента   ┆ ---    ┆   ┆ LP        ┆ NLP       ┆ NLP       ┆ P         │\n",
       "│ ---        ┆ f64       ┆ ---        ┆ f64    ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ str        ┆           ┆ f64        ┆        ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪════════════╪════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ C          ┆ 113.0     ┆ 1493.0     ┆ 1.0    ┆ … ┆ 1.00001   ┆ 0.010663  ┆ 0.00001   ┆ 0.00001   │\n",
       "│ C          ┆ 113.0     ┆ 5580.0     ┆ 5.0    ┆ … ┆ 0.437833  ┆ 0.056662  ┆ 0.140346  ┆ 0.051855  │\n",
       "│ E          ┆ 126.0     ┆ 5619.0     ┆ 5.0    ┆ … ┆ 0.00001   ┆ 0.00001   ┆ 0.00001   ┆ 1.00001   │\n",
       "│ E          ┆ 123.0     ┆ 310.0      ┆ 3.0    ┆ … ┆ 0.930468  ┆ 0.025189  ┆ 0.119213  ┆ 0.000378  │\n",
       "│ E          ┆ 123.0     ┆ 1913.0     ┆ 5.0    ┆ … ┆ 0.069552  ┆ 0.217348  ┆ 0.019134  ┆ 0.507822  │\n",
       "└────────────┴───────────┴────────────┴────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pl.read_csv(\"preprocessing.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_258, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Тег</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;VC2 S1 S4&quot;</td><td>1</td></tr><tr><td>&quot;VC2 VP2 H2 VC4…</td><td>1</td></tr><tr><td>&quot; VC1 H1 VC3 H3…</td><td>1</td></tr><tr><td>&quot;VP3 H3 LMS VC3…</td><td>1</td></tr><tr><td>&quot;VC3 VC2 H1&quot;</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;VC2  VC4&quot;</td><td>1</td></tr><tr><td>&quot;S3 VP3 VC3 VP2…</td><td>1</td></tr><tr><td>&quot;S3 H3 VC3 VP2&quot;</td><td>1</td></tr><tr><td>&quot;VP3 T3&quot;</td><td>3</td></tr><tr><td>&quot;S3 &quot;</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_258, 2)\n",
       "┌────────────────────┬───────┐\n",
       "│ Тег                ┆ count │\n",
       "│ ---                ┆ ---   │\n",
       "│ str                ┆ u32   │\n",
       "╞════════════════════╪═══════╡\n",
       "│ VC2 S1 S4          ┆ 1     │\n",
       "│ VC2 VP2 H2 VC4     ┆ 1     │\n",
       "│  VC1 H1 VC3 H3     ┆ 1     │\n",
       "│ VP3 H3 LMS VC3     ┆ 1     │\n",
       "│ VC3 VC2 H1         ┆ 2     │\n",
       "│ …                  ┆ …     │\n",
       "│ VC2  VC4           ┆ 1     │\n",
       "│ S3 VP3 VC3 VP2 VC2 ┆ 1     │\n",
       "│ S3 H3 VC3 VP2      ┆ 1     │\n",
       "│ VP3 T3             ┆ 3     │\n",
       "│ S3                 ┆ 6     │\n",
       "└────────────────────┴───────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Тег\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(\n",
    "    (pl.col(\"Тег\").apply(lambda x: \" \".join(re.findall(r\"[A-Z]{1,2}\\d|LMS\", x)))).alias(\"corrected_tag\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_filter = (\n",
    "    (pl.col(\"corrected_tag\").eq(\"\"))\n",
    ")\n",
    "\n",
    "dataset = dataset.filter(~null_filter)\n",
    "dataset = dataset.filter(~(pl.col(\"Комментарий\").is_null()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(\n",
    "    pl.col(\"corrected_tag\")\n",
    "    .str.replace_all(r\"VC4|VP4|VC5|S4|T4|H4|EA1\", \"\")\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\s\\s+\", \" \")\n",
    "    .str.replace(r\"GH3\", \"H3\")\n",
    "    .str.replace(r\"HH3\", \"H3\")\n",
    "    .str.replace(r\"BP3\", \"VP3\")\n",
    "    .str.replace(r\"V3\", \"VC3\")\n",
    "    .str.replace(r\"V2\", \"VP2\")\n",
    ")\n",
    "\n",
    "dataset = dataset.filter(~(pl.col(\"corrected_tag\").eq(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (17, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>corrected_tag</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;H3&quot;</td><td>20391</td></tr><tr><td>&quot;VC2&quot;</td><td>14414</td></tr><tr><td>&quot;VC3&quot;</td><td>8111</td></tr><tr><td>&quot;VP3&quot;</td><td>4972</td></tr><tr><td>&quot;VP2&quot;</td><td>4897</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;LMS&quot;</td><td>669</td></tr><tr><td>&quot;T2&quot;</td><td>548</td></tr><tr><td>&quot;T1&quot;</td><td>399</td></tr><tr><td>&quot;T3&quot;</td><td>234</td></tr><tr><td>&quot;E2&quot;</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (17, 2)\n",
       "┌───────────────┬───────┐\n",
       "│ corrected_tag ┆ count │\n",
       "│ ---           ┆ ---   │\n",
       "│ str           ┆ u32   │\n",
       "╞═══════════════╪═══════╡\n",
       "│ H3            ┆ 20391 │\n",
       "│ VC2           ┆ 14414 │\n",
       "│ VC3           ┆ 8111  │\n",
       "│ VP3           ┆ 4972  │\n",
       "│ VP2           ┆ 4897  │\n",
       "│ …             ┆ …     │\n",
       "│ LMS           ┆ 669   │\n",
       "│ T2            ┆ 548   │\n",
       "│ T1            ┆ 399   │\n",
       "│ T3            ┆ 234   │\n",
       "│ E2            ┆ 1     │\n",
       "└───────────────┴───────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"corrected_tag\"].str.split(by = \" \").explode().value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(~pl.col(\"corrected_tag\").str.contains(\"E2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>corrected_tag</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;VC&quot;</td><td>26304</td></tr><tr><td>&quot;H&quot;</td><td>24437</td></tr><tr><td>&quot;VP&quot;</td><td>11560</td></tr><tr><td>&quot;S&quot;</td><td>1973</td></tr><tr><td>&quot;E&quot;</td><td>1785</td></tr><tr><td>&quot;T&quot;</td><td>1181</td></tr><tr><td>&quot;LMS&quot;</td><td>669</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 2)\n",
       "┌───────────────┬───────┐\n",
       "│ corrected_tag ┆ count │\n",
       "│ ---           ┆ ---   │\n",
       "│ str           ┆ u32   │\n",
       "╞═══════════════╪═══════╡\n",
       "│ VC            ┆ 26304 │\n",
       "│ H             ┆ 24437 │\n",
       "│ VP            ┆ 11560 │\n",
       "│ S             ┆ 1973  │\n",
       "│ E             ┆ 1785  │\n",
       "│ T             ┆ 1181  │\n",
       "│ LMS           ┆ 669   │\n",
       "└───────────────┴───────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_sub_tags(tags: str):\n",
    "    split = tags.split(sep=\" \")\n",
    "    new_tag = [x[:-1] if x[-1].isdigit() else x for x in split]\n",
    "    return \" \".join(new_tag)\n",
    "\n",
    "dataset = dataset.with_columns(\n",
    "    pl.col(\"corrected_tag\").apply(remove_sub_tags)\n",
    ")\n",
    "\n",
    "dataset[\"corrected_tag\"].str.split(by = \" \").explode().value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset[\"corrected_tag\"].str.split(by = \" \").explode().unique().sort().to_list()\n",
    "target = dict(zip(target, range(len(target))))\n",
    "reverse_target = {v : k for k, v in target.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tags: str) -> list[float]:\n",
    "    \"\"\"Turn str with tags into list with digit labels.\n",
    "\n",
    "    Args:\n",
    "        tags (str): tag text representation.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: numeric labels.\n",
    "    \"\"\"\n",
    "    split = tags.split(sep = \" \")\n",
    "    res = np.zeros(len(target))\n",
    "    for x in split:\n",
    "        res[target[x]] = 1\n",
    "    return res.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_columns(pl.col(\"corrected_tag\").apply(vectorize).alias(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (54_648, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Направление</th><th>Факультет</th><th>ID студента</th><th>Оценка</th><th>Категория</th><th>Тег</th><th>Комментарий</th><th>Статус</th><th>Neutral</th><th>Positive</th><th>Negative</th><th>Exclamations</th><th>have_code</th><th>Neutral_NLP</th><th>Positive_NLP</th><th>Negative_NLP</th><th>Speech_NLP</th><th>corrected_tag</th><th>labels</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;C&quot;</td><td>113.0</td><td>1493.0</td><td>1.0</td><td>&quot;Видео&quot;</td><td>&quot;VP2&quot;</td><td>&quot;Видео лагает&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>1.00001</td><td>0.010663</td><td>0.00001</td><td>0.00001</td><td>&quot;VP&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr><tr><td>&quot;C&quot;</td><td>113.0</td><td>5580.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Торгом Бабаян!…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5</td><td>false</td><td>0.437833</td><td>0.056662</td><td>0.140346</td><td>0.051855</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>126.0</td><td>5619.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3&quot;</td><td>&quot;Спасибо)&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.00001</td><td>0.00001</td><td>0.00001</td><td>1.00001</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>310.0</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2 E1&quot;</td><td>&quot;комментарий со…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.930468</td><td>0.025189</td><td>0.119213</td><td>0.000378</td><td>&quot;H E&quot;</td><td>[1.0, 1.0, … 0.0]</td></tr><tr><td>&quot;E&quot;</td><td>123.0</td><td>1913.0</td><td>5.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H3 D&quot;</td><td>&quot;Жонибек, хочу …</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2</td><td>false</td><td>0.069552</td><td>0.217348</td><td>0.019134</td><td>0.507822</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Z&quot;</td><td>133.0</td><td>null</td><td>3.0</td><td>&quot;ДЗ&quot;</td><td>&quot;H2&quot;</td><td>&quot;требуемый форм…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.822199</td><td>0.013647</td><td>0.020974</td><td>0.00001</td><td>&quot;H&quot;</td><td>[0.0, 1.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>&quot;S1&quot;</td><td>&quot;заплатила и да…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.228166</td><td>0.042098</td><td>0.644235</td><td>0.006108</td><td>&quot;S&quot;</td><td>[0.0, 0.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>7.0</td><td>null</td><td>&quot;LMS&quot;</td><td>&quot;Крайне раздраж…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.692652</td><td>0.073706</td><td>0.262852</td><td>0.00523</td><td>&quot;LMS&quot;</td><td>[0.0, 0.0, … 0.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;VC2 VP2&quot;</td><td>&quot;Аналитик данны…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>true</td><td>0.705795</td><td>0.053413</td><td>0.320831</td><td>0.001511</td><td>&quot;VC VP&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr><tr><td>&quot;Z&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;VP2 VC2&quot;</td><td>&quot;Системный анал…</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td><td>false</td><td>0.679189</td><td>0.092698</td><td>0.156115</td><td>0.00408</td><td>&quot;VP VC&quot;</td><td>[0.0, 0.0, … 1.0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (54_648, 19)\n",
       "┌────────────┬───────────┬────────────┬────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ Направлени ┆ Факультет ┆ ID         ┆ Оценка ┆ … ┆ Negative_ ┆ Speech_NL ┆ corrected ┆ labels    │\n",
       "│ е          ┆ ---       ┆ студента   ┆ ---    ┆   ┆ NLP       ┆ P         ┆ _tag      ┆ ---       │\n",
       "│ ---        ┆ f64       ┆ ---        ┆ f64    ┆   ┆ ---       ┆ ---       ┆ ---       ┆ list[f64] │\n",
       "│ str        ┆           ┆ f64        ┆        ┆   ┆ f64       ┆ f64       ┆ str       ┆           │\n",
       "╞════════════╪═══════════╪════════════╪════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ C          ┆ 113.0     ┆ 1493.0     ┆ 1.0    ┆ … ┆ 0.00001   ┆ 0.00001   ┆ VP        ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "│ C          ┆ 113.0     ┆ 5580.0     ┆ 5.0    ┆ … ┆ 0.140346  ┆ 0.051855  ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 126.0     ┆ 5619.0     ┆ 5.0    ┆ … ┆ 0.00001   ┆ 1.00001   ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 123.0     ┆ 310.0      ┆ 3.0    ┆ … ┆ 0.119213  ┆ 0.000378  ┆ H E       ┆ [1.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ E          ┆ 123.0     ┆ 1913.0     ┆ 5.0    ┆ … ┆ 0.019134  ┆ 0.507822  ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ …          ┆ …         ┆ …          ┆ …      ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ Z          ┆ 133.0     ┆ null       ┆ 3.0    ┆ … ┆ 0.020974  ┆ 0.00001   ┆ H         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ 0.0    ┆ … ┆ 0.644235  ┆ 0.006108  ┆ S         ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ 7.0    ┆ … ┆ 0.262852  ┆ 0.00523   ┆ LMS       ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ null   ┆ … ┆ 0.320831  ┆ 0.001511  ┆ VC VP     ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "│ Z          ┆ null      ┆ null       ┆ null   ┆ … ┆ 0.156115  ┆ 0.00408   ┆ VP VC     ┆ [0.0,     │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 0.0, …    │\n",
       "│            ┆           ┆            ┆        ┆   ┆           ┆           ┆           ┆ 1.0]      │\n",
       "└────────────┴───────────┴────────────┴────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dataset = dataset.select(\n",
    "    pl.col(\"Комментарий\"),\n",
    "    pl.col(\"Направление\"),\n",
    "    pl.col(\"Факультет\"),\n",
    "    pl.col(\"Оценка\"),\n",
    "    pl.col(\"Neutral\"),\n",
    "    pl.col(\"Positive\"),\n",
    "    pl.col(\"Negative\"),\n",
    "    pl.col(\"Exclamations\"),\n",
    "    pl.col(\"have_code\"),\n",
    "    pl.col(\"Neutral_NLP\"),\n",
    "    pl.col(\"Positive_NLP\"),\n",
    "    pl.col(\"Negative_NLP\"),\n",
    "    pl.col(\"Speech_NLP\"),\n",
    "    pl.col(\"corrected_tag\"),\n",
    "    pl.col(\"labels\"),\n",
    "    pl.col(\"corrected_tag\").str.split(by=\" \").alias(\"temp\"),\n",
    ")\n",
    "clear_dataset = clear_dataset.explode(columns=[\"temp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    clear_dataset,\n",
    "    test_size=0.1,\n",
    "    random_state=3317,\n",
    "    stratify=clear_dataset[\"temp\"],\n",
    ")\n",
    "\n",
    "train_df = train_df.drop(columns=[\"corrected_tag\", \"temp\"])\n",
    "test_df = test_df.drop(columns=[\"corrected_tag\", \"temp\"])\n",
    "\n",
    "train_df = train_df.rename({\"Комментарий\": \"text\"})\n",
    "test_df = test_df.rename({\"Комментарий\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df.to_pandas(), split=\"train\")\n",
    "test_dataset = Dataset.from_pandas(test_df.to_pandas(), split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "def preprocess_data(sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Encode input text into sequence of tokens.\n",
    "    Also add corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        sample (dict[str, Any]): raw input text.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]: transformed sample with tokenized text and labels.\n",
    "    \"\"\"\n",
    "    text = sample[\"text\"]\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoding[\"labels\"] = sample[\"labels\"]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5bdd48eb0b40bfb4ffdfc7a3d7787b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a92713787848edb864918bb691ec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6791 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_train = train_dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "encoded_test = test_dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=test_dataset.column_names\n",
    ")\n",
    "encoded_train.set_format(\"torch\")\n",
    "encoded_test.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(\n",
    "    predictions: np.ndarray, labels: np.ndarray, threshold: float = 0.5\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Compute mltilabel metrics.\n",
    "\n",
    "    Args:\n",
    "        predictions (np.ndarray): logits array\n",
    "        labels (np.ndarray): labels array\n",
    "        threshold (float, optional): activation threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: metrics dict\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    f1_micro_average = f1_score(y_true=labels, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(labels, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> dict[str, float]:\n",
    "    \"\"\"Metrics computation wrapper.\n",
    "\n",
    "    Args:\n",
    "        p (EvalPrediction): hf model output\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: metrics dict\n",
    "    \"\"\"\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_pipeline(\n",
    "    exp_name: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    train_dataset: Dataset,\n",
    "    eval_dataset: Dataset,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 2e-5,\n",
    "    epochs_num: int = 20,\n",
    "    model_name=None\n",
    ") -> Trainer:\n",
    "    \"\"\"Training process wrapper.\n",
    "\n",
    "    Args:\n",
    "        exp_name (str): name of the local folder\n",
    "        for saving model checkpoints.\n",
    "        tokenizer (AutoTokenizer): model tokenizer\n",
    "        train_dataset (Dataset): train dataset split\n",
    "        eval_dataset (Dataset): test dataset split\n",
    "        batch_size (int, optional): number of samples\n",
    "        in sigle batch. Defaults to 32.\n",
    "        lr (float, optional): model's learning rate. Defaults to 2e-5.\n",
    "        epochs_num (int, optional):\n",
    "        number of training iterations. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: hf training pipeline abstraction class.\n",
    "    \"\"\"\n",
    "    args = TrainingArguments(\n",
    "        exp_name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs_num,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"cointegrated/rubert-tiny2\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(target),\n",
    "        id2label=target,\n",
    "        label2id=reverse_target\n",
    "    )\n",
    "    if model_name is not None:\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 180\n",
    "EPOCHS = 50\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = make_training_pipeline(\"f_without_focal\", tokenizer, encoded_train, encoded_test, batch_size=BATCH_SIZE, epochs_num=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7480' max='8500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7480/8500 1:08:24 < 09:19, 1.82 it/s, Epoch 44/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.346003</td>\n",
       "      <td>0.616881</td>\n",
       "      <td>0.736454</td>\n",
       "      <td>0.438374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.271092</td>\n",
       "      <td>0.741702</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.554263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.235977</td>\n",
       "      <td>0.779079</td>\n",
       "      <td>0.853521</td>\n",
       "      <td>0.582094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.216731</td>\n",
       "      <td>0.797074</td>\n",
       "      <td>0.866240</td>\n",
       "      <td>0.609630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.204721</td>\n",
       "      <td>0.805586</td>\n",
       "      <td>0.870827</td>\n",
       "      <td>0.622441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.196783</td>\n",
       "      <td>0.811218</td>\n",
       "      <td>0.872720</td>\n",
       "      <td>0.633044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.190647</td>\n",
       "      <td>0.817693</td>\n",
       "      <td>0.878968</td>\n",
       "      <td>0.643351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.185663</td>\n",
       "      <td>0.821176</td>\n",
       "      <td>0.880308</td>\n",
       "      <td>0.649831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.181578</td>\n",
       "      <td>0.828007</td>\n",
       "      <td>0.886296</td>\n",
       "      <td>0.659255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.177097</td>\n",
       "      <td>0.832052</td>\n",
       "      <td>0.887559</td>\n",
       "      <td>0.667648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.174425</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.891796</td>\n",
       "      <td>0.673833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.170941</td>\n",
       "      <td>0.839252</td>\n",
       "      <td>0.893002</td>\n",
       "      <td>0.678987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.168634</td>\n",
       "      <td>0.843630</td>\n",
       "      <td>0.897224</td>\n",
       "      <td>0.684435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.166075</td>\n",
       "      <td>0.844571</td>\n",
       "      <td>0.896093</td>\n",
       "      <td>0.691356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.164184</td>\n",
       "      <td>0.849157</td>\n",
       "      <td>0.900212</td>\n",
       "      <td>0.696952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.161902</td>\n",
       "      <td>0.850776</td>\n",
       "      <td>0.901338</td>\n",
       "      <td>0.699455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.161076</td>\n",
       "      <td>0.851946</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>0.704315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.159904</td>\n",
       "      <td>0.855268</td>\n",
       "      <td>0.906173</td>\n",
       "      <td>0.713297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.158957</td>\n",
       "      <td>0.855135</td>\n",
       "      <td>0.906551</td>\n",
       "      <td>0.711383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.157594</td>\n",
       "      <td>0.858318</td>\n",
       "      <td>0.909076</td>\n",
       "      <td>0.719776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.157526</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>0.910565</td>\n",
       "      <td>0.722574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.157332</td>\n",
       "      <td>0.860405</td>\n",
       "      <td>0.910972</td>\n",
       "      <td>0.725372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.156802</td>\n",
       "      <td>0.862029</td>\n",
       "      <td>0.912840</td>\n",
       "      <td>0.728317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.155507</td>\n",
       "      <td>0.864015</td>\n",
       "      <td>0.914397</td>\n",
       "      <td>0.732145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.155069</td>\n",
       "      <td>0.865352</td>\n",
       "      <td>0.915066</td>\n",
       "      <td>0.735091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.156129</td>\n",
       "      <td>0.865308</td>\n",
       "      <td>0.916069</td>\n",
       "      <td>0.736858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.155459</td>\n",
       "      <td>0.866489</td>\n",
       "      <td>0.916307</td>\n",
       "      <td>0.737447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.155076</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>0.918293</td>\n",
       "      <td>0.741864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.154261</td>\n",
       "      <td>0.868073</td>\n",
       "      <td>0.917329</td>\n",
       "      <td>0.743779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.154698</td>\n",
       "      <td>0.869275</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>0.744809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.155139</td>\n",
       "      <td>0.871592</td>\n",
       "      <td>0.920637</td>\n",
       "      <td>0.751583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.871782</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.751141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.154694</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.920872</td>\n",
       "      <td>0.751141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.154617</td>\n",
       "      <td>0.873615</td>\n",
       "      <td>0.922494</td>\n",
       "      <td>0.755412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.155110</td>\n",
       "      <td>0.872438</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.752466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.155645</td>\n",
       "      <td>0.874799</td>\n",
       "      <td>0.924322</td>\n",
       "      <td>0.757620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.155161</td>\n",
       "      <td>0.873987</td>\n",
       "      <td>0.923238</td>\n",
       "      <td>0.756590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.154584</td>\n",
       "      <td>0.875221</td>\n",
       "      <td>0.923985</td>\n",
       "      <td>0.758798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.154256</td>\n",
       "      <td>0.876239</td>\n",
       "      <td>0.924952</td>\n",
       "      <td>0.760713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.155255</td>\n",
       "      <td>0.875045</td>\n",
       "      <td>0.924323</td>\n",
       "      <td>0.758946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.154149</td>\n",
       "      <td>0.877291</td>\n",
       "      <td>0.925398</td>\n",
       "      <td>0.762922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.876217</td>\n",
       "      <td>0.925513</td>\n",
       "      <td>0.761743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.155040</td>\n",
       "      <td>0.875999</td>\n",
       "      <td>0.924847</td>\n",
       "      <td>0.761596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.154932</td>\n",
       "      <td>0.876661</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>0.762185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7480, training_loss=0.12872666955631684, metrics={'train_runtime': 4105.645, 'train_samples_per_second': 744.317, 'train_steps_per_second': 2.07, 'total_flos': 1.984358718079795e+16, 'train_loss': 0.12872666955631684, 'epoch': 44.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15414921939373016,\n",
       " 'eval_f1': 0.8772913745650932,\n",
       " 'eval_roc_auc': 0.9253983535608915,\n",
       " 'eval_accuracy': 0.7629215137682226,\n",
       " 'eval_runtime': 4.2929,\n",
       " 'eval_samples_per_second': 1581.908,\n",
       " 'eval_steps_per_second': 4.426,\n",
       " 'epoch': 44.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = trainer.predict(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.8772913745650932, 'roc_auc': 0.9253983535608915, 'accuracy': 0.7629215137682226}\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = make_training_pipeline(\"v\", tokenizer, encoded_train, encoded_test, batch_size=BATCH_SIZE, epochs_num=EPOCHS, lr=LR, model_name=\"f/checkpoint-14340/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_preds = trainer.predict(encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_preds = trainer.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделать сравнение с Focal Loss и без него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'Направление', 'Факультет', 'Оценка', 'Neutral', 'Positive', 'Negative', 'Exclamations', 'have_code', 'Neutral_NLP', 'Positive_NLP', 'Negative_NLP', 'Speech_NLP', 'labels'],\n",
       "    num_rows: 61118\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = pd.get_dummies(train_df.to_pandas()[\"Направление\"])\n",
    "departments = pd.get_dummies(train_df.to_pandas()[\"Факультет\"])\n",
    "meta_dataset_train = train_df.select(pl.exclude(\"Направление\", \"Факультет\")).to_pandas()\n",
    "meta_dataset_train = pd.concat([meta_dataset_train, directions, departments, pd.DataFrame(train_preds.predictions)], axis=1)\n",
    "meta_dataset_train = meta_dataset_train.drop(columns=[\"text\"])\n",
    "\n",
    "meta_dataset_test = test_df.select(pl.exclude(\"Направление\", \"Факультет\")).to_pandas()\n",
    "meta_dataset_test = pd.concat([meta_dataset_test, directions, departments, pd.DataFrame(test_preds.predictions)], axis=1)\n",
    "meta_dataset_test = meta_dataset_test.drop(columns=[\"text\"])\n",
    "\n",
    "meta_dataset_test = meta_dataset_test.dropna(subset=[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = meta_dataset_train.drop('labels', axis=1), np.array(meta_dataset_train[\"labels\"].to_list())\n",
    "X_test, y_test = meta_dataset_test.drop('labels', axis=1), np.array(meta_dataset_test[\"labels\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train)\n",
    "test_pool = Pool(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=trial.suggest_int(\"iterations\", 500, 2000),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1e-8, 100.0, log=True),\n",
    "        bootstrap_type=trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\"]),\n",
    "        random_strength=trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "        bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "        od_type=trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n",
    "        od_wait=trial.suggest_int(\"od_wait\", 10, 50),\n",
    "        verbose=False,\n",
    "        task_type=\"GPU\",\n",
    "        devices='0',\n",
    "        loss_function = trial.suggest_categorical(\"loss_function\", [\"MultiCrossEntropy\", \"MultiLogloss\"]))\n",
    "    model.fit(train_pool, eval_set=test_pool)\n",
    "    y_pred = model.predict(test_pool)\n",
    "    return hamming_loss(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-22 16:42:19,114] A new study created in memory with name: catboost\n",
      "Warning: less than 75% gpu memory available for training. Free: 2568.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:42:28,049] Trial 0 finished with value: 0.04989797420956308 and parameters: {'iterations': 893, 'learning_rate': 0.0020766721769608226, 'depth': 5, 'l2_leaf_reg': 0.00039189423261207457, 'bootstrap_type': 'Bayesian', 'random_strength': 7.74470472863429e-06, 'bagging_temperature': 5.183928205975371, 'od_type': 'Iter', 'od_wait': 40, 'loss_function': 'MultiLogloss'}. Best is trial 0 with value: 0.04989797420956308.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:09,065] Trial 1 finished with value: 0.04935103182783937 and parameters: {'iterations': 1443, 'learning_rate': 0.0017787538482591226, 'depth': 10, 'l2_leaf_reg': 0.00027055071527787685, 'bootstrap_type': 'Bayesian', 'random_strength': 0.12765125185466167, 'bagging_temperature': 7.941185757915866, 'od_type': 'Iter', 'od_wait': 33, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:10,584] Trial 2 finished with value: 0.04935103182783937 and parameters: {'iterations': 932, 'learning_rate': 0.021899677346699262, 'depth': 7, 'l2_leaf_reg': 6.105015510183205e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 5.227231242013996e-05, 'bagging_temperature': 1.991952352050984, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:12,952] Trial 3 finished with value: 0.050381807854934055 and parameters: {'iterations': 1256, 'learning_rate': 0.010494020147150274, 'depth': 4, 'l2_leaf_reg': 0.20744390859184064, 'bootstrap_type': 'Bayesian', 'random_strength': 8.704530653536676, 'bagging_temperature': 1.6287753326976728, 'od_type': 'Iter', 'od_wait': 38, 'loss_function': 'MultiLogloss'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:20,363] Trial 4 finished with value: 0.05000315543681764 and parameters: {'iterations': 588, 'learning_rate': 0.03790287646860259, 'depth': 6, 'l2_leaf_reg': 0.10598324121909684, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0002636705926743837, 'bagging_temperature': 9.728723298688061, 'od_type': 'IncToDec', 'od_wait': 33, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:56,300] Trial 5 finished with value: 0.049982119191366726 and parameters: {'iterations': 521, 'learning_rate': 0.0015069197830871604, 'depth': 10, 'l2_leaf_reg': 54.274270357483914, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0002650536373654029, 'bagging_temperature': 3.409359716532486, 'od_type': 'IncToDec', 'od_wait': 41, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:43:56,964] Trial 6 finished with value: 0.05061320655489408 and parameters: {'iterations': 1500, 'learning_rate': 0.06641173145737062, 'depth': 5, 'l2_leaf_reg': 12.758016626212465, 'bootstrap_type': 'Bayesian', 'random_strength': 8.302734146614306e-06, 'bagging_temperature': 7.00298420049446, 'od_type': 'Iter', 'od_wait': 17, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:44:15,463] Trial 7 finished with value: 0.05006626417317037 and parameters: {'iterations': 1094, 'learning_rate': 0.03835755070376457, 'depth': 7, 'l2_leaf_reg': 0.1735717250993288, 'bootstrap_type': 'Bayesian', 'random_strength': 0.13715362597057945, 'bagging_temperature': 3.5152409979850177, 'od_type': 'IncToDec', 'od_wait': 46, 'loss_function': 'MultiLogloss'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:44:42,370] Trial 8 finished with value: 0.04935103182783937 and parameters: {'iterations': 1856, 'learning_rate': 0.01849682603070215, 'depth': 7, 'l2_leaf_reg': 2.3719615831810452e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 3.30016207773554, 'bagging_temperature': 5.594301234747533, 'od_type': 'IncToDec', 'od_wait': 27, 'loss_function': 'MultiLogloss'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:44:47,694] Trial 9 finished with value: 0.050087300418621285 and parameters: {'iterations': 688, 'learning_rate': 0.023297333198997773, 'depth': 4, 'l2_leaf_reg': 2.1798631044417786e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 0.04245980204881909, 'bagging_temperature': 9.342098037151883, 'od_type': 'IncToDec', 'od_wait': 45, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:08,366] Trial 10 finished with value: 0.04935103182783937 and parameters: {'iterations': 1990, 'learning_rate': 0.003906598333809237, 'depth': 10, 'l2_leaf_reg': 0.0002806556189391769, 'bootstrap_type': 'Bayesian', 'random_strength': 3.6797423554993524e-08, 'bagging_temperature': 7.641974455103036, 'od_type': 'Iter', 'od_wait': 26, 'loss_function': 'MultiCrossEntropy'}. Best is trial 1 with value: 0.04935103182783937.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:18,698] Trial 11 finished with value: 0.04907756063697751 and parameters: {'iterations': 1499, 'learning_rate': 0.004763255893797943, 'depth': 9, 'l2_leaf_reg': 2.397960686461151e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0045692725527288975, 'bagging_temperature': 0.3451938609825189, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 11 with value: 0.04907756063697751.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:29,753] Trial 12 finished with value: 0.04897237940972295 and parameters: {'iterations': 1552, 'learning_rate': 0.004460820625672682, 'depth': 9, 'l2_leaf_reg': 6.556035169186954e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.04014877820488786, 'bagging_temperature': 0.21558094719565496, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:41,149] Trial 13 finished with value: 0.04907756063697751 and parameters: {'iterations': 1660, 'learning_rate': 0.004369249900798397, 'depth': 9, 'l2_leaf_reg': 1.590357617120062e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.014105178146398093, 'bagging_temperature': 0.13203441887856737, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:50,352] Trial 14 finished with value: 0.0490565243915266 and parameters: {'iterations': 1687, 'learning_rate': 0.005672211129117697, 'depth': 9, 'l2_leaf_reg': 3.331242330100906e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.005148123375123526, 'bagging_temperature': 0.1953650102537079, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:54,452] Trial 15 finished with value: 0.049266886846035716 and parameters: {'iterations': 1738, 'learning_rate': 0.008421122455382107, 'depth': 8, 'l2_leaf_reg': 3.169937598527381e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 0.002697761565081663, 'bagging_temperature': 1.568410798411084, 'od_type': 'Iter', 'od_wait': 18, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:45:58,824] Trial 16 finished with value: 0.04922481435513389 and parameters: {'iterations': 1352, 'learning_rate': 0.007366075367810421, 'depth': 8, 'l2_leaf_reg': 2.054159810722931e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 0.8778728761738366, 'bagging_temperature': 3.3337733232548605, 'od_type': 'Iter', 'od_wait': 22, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:46:15,204] Trial 17 finished with value: 0.048993415655173865 and parameters: {'iterations': 1661, 'learning_rate': 0.002829457302924841, 'depth': 9, 'l2_leaf_reg': 8.565130751457636e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 3.151265829650291e-07, 'bagging_temperature': 0.964619999147061, 'od_type': 'Iter', 'od_wait': 12, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:46:27,550] Trial 18 finished with value: 0.04922481435513389 and parameters: {'iterations': 1987, 'learning_rate': 0.0025261669223620104, 'depth': 8, 'l2_leaf_reg': 0.007368565408799676, 'bootstrap_type': 'Bayesian', 'random_strength': 2.130476731754271e-08, 'bagging_temperature': 2.6458413030879506, 'od_type': 'Iter', 'od_wait': 15, 'loss_function': 'MultiLogloss'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:47:13,397] Trial 19 finished with value: 0.0490565243915266 and parameters: {'iterations': 1607, 'learning_rate': 0.0010158034653571734, 'depth': 9, 'l2_leaf_reg': 6.399864664344608e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 2.025267765328559e-07, 'bagging_temperature': 4.221961323833117, 'od_type': 'Iter', 'od_wait': 22, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:47:16,291] Trial 20 finished with value: 0.04916170561878116 and parameters: {'iterations': 1822, 'learning_rate': 0.012543679223791139, 'depth': 8, 'l2_leaf_reg': 3.8845540634365975e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 5.476026018010231e-07, 'bagging_temperature': 1.0484843475553163, 'od_type': 'Iter', 'od_wait': 14, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:47:31,344] Trial 21 finished with value: 0.04907756063697751 and parameters: {'iterations': 1640, 'learning_rate': 0.003164205953574395, 'depth': 9, 'l2_leaf_reg': 1.2983864934560903e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 0.001440195736999006, 'bagging_temperature': 0.07935420558831577, 'od_type': 'Iter', 'od_wait': 10, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:47:39,989] Trial 22 finished with value: 0.0490565243915266 and parameters: {'iterations': 1798, 'learning_rate': 0.005371392388920908, 'depth': 9, 'l2_leaf_reg': 6.57481263985254e-08, 'bootstrap_type': 'Bayesian', 'random_strength': 0.45163051521603476, 'bagging_temperature': 0.9596076381694345, 'od_type': 'Iter', 'od_wait': 14, 'loss_function': 'MultiCrossEntropy'}. Best is trial 12 with value: 0.04897237940972295.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:47:51,797] Trial 23 finished with value: 0.04825714706439195 and parameters: {'iterations': 1344, 'learning_rate': 0.006323059213654237, 'depth': 10, 'l2_leaf_reg': 2.4657036220063024e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 2.435027987341135e-05, 'bagging_temperature': 2.254128611426352, 'od_type': 'Iter', 'od_wait': 13, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:48:14,704] Trial 24 finished with value: 0.048320255800744685 and parameters: {'iterations': 1192, 'learning_rate': 0.003156359991903609, 'depth': 10, 'l2_leaf_reg': 7.605195046544202e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 1.5117459360660322e-06, 'bagging_temperature': 2.3454660698964145, 'od_type': 'Iter', 'od_wait': 22, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:48:25,215] Trial 25 finished with value: 0.048320255800744685 and parameters: {'iterations': 1135, 'learning_rate': 0.006534304309104747, 'depth': 10, 'l2_leaf_reg': 5.975341149140025e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 5.337419639331852e-06, 'bagging_temperature': 2.408532361188642, 'od_type': 'Iter', 'od_wait': 22, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:49:22,985] Trial 26 finished with value: 0.04848854576435198 and parameters: {'iterations': 1153, 'learning_rate': 0.013049228508749814, 'depth': 10, 'l2_leaf_reg': 3.397388783679863e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 3.5813875189047353e-06, 'bagging_temperature': 2.486052005438321, 'od_type': 'IncToDec', 'od_wait': 21, 'loss_function': 'MultiLogloss'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:49:34,232] Trial 27 finished with value: 0.04861476323705745 and parameters: {'iterations': 1009, 'learning_rate': 0.0074585392208961175, 'depth': 10, 'l2_leaf_reg': 6.31507616361141e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 2.6676620238227264e-05, 'bagging_temperature': 4.402843374794523, 'od_type': 'Iter', 'od_wait': 27, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:50:36,527] Trial 28 finished with value: 0.04916170561878116 and parameters: {'iterations': 1294, 'learning_rate': 0.0011698391566876774, 'depth': 10, 'l2_leaf_reg': 6.0614377701551905e-06, 'bootstrap_type': 'Bayesian', 'random_strength': 1.3953508347487627e-06, 'bagging_temperature': 6.132096576634117, 'od_type': 'Iter', 'od_wait': 24, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:51:09,616] Trial 29 finished with value: 0.048446473273450155 and parameters: {'iterations': 1109, 'learning_rate': 0.0023553901295097243, 'depth': 10, 'l2_leaf_reg': 0.004181809287689663, 'bootstrap_type': 'Bayesian', 'random_strength': 5.134423231736134e-05, 'bagging_temperature': 2.753154929465881, 'od_type': 'Iter', 'od_wait': 31, 'loss_function': 'MultiLogloss'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:51:33,811] Trial 30 finished with value: 0.04878305320066475 and parameters: {'iterations': 1346, 'learning_rate': 0.0032631228740463056, 'depth': 10, 'l2_leaf_reg': 9.896820004743802e-05, 'bootstrap_type': 'Bayesian', 'random_strength': 9.478631862415288e-08, 'bagging_temperature': 4.582187201766905, 'od_type': 'Iter', 'od_wait': 19, 'loss_function': 'MultiCrossEntropy'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:52:12,941] Trial 31 finished with value: 0.04846750951890107 and parameters: {'iterations': 1129, 'learning_rate': 0.002086116431251068, 'depth': 10, 'l2_leaf_reg': 0.005621425011733224, 'bootstrap_type': 'Bayesian', 'random_strength': 4.469993552582349e-05, 'bagging_temperature': 2.7428036564361875, 'od_type': 'Iter', 'od_wait': 31, 'loss_function': 'MultiLogloss'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:52:52,564] Trial 32 finished with value: 0.04827818330984286 and parameters: {'iterations': 815, 'learning_rate': 0.001924183682555033, 'depth': 10, 'l2_leaf_reg': 0.004552832236334372, 'bootstrap_type': 'Bayesian', 'random_strength': 5.096874892745769e-06, 'bagging_temperature': 2.1247124073338233, 'od_type': 'Iter', 'od_wait': 29, 'loss_function': 'MultiLogloss'}. Best is trial 23 with value: 0.04825714706439195.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:53:44,487] Trial 33 finished with value: 0.04821507457349012 and parameters: {'iterations': 768, 'learning_rate': 0.0015525247937596375, 'depth': 10, 'l2_leaf_reg': 0.0010450462421863532, 'bootstrap_type': 'Bayesian', 'random_strength': 6.775703358551985e-06, 'bagging_temperature': 2.1236588035208244, 'od_type': 'Iter', 'od_wait': 36, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:54:36,185] Trial 34 finished with value: 0.04840440078254833 and parameters: {'iterations': 842, 'learning_rate': 0.0015634150643064253, 'depth': 10, 'l2_leaf_reg': 0.04048185094266786, 'bootstrap_type': 'Bayesian', 'random_strength': 1.7793887709324463e-06, 'bagging_temperature': 1.8263489055649593, 'od_type': 'Iter', 'od_wait': 37, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:54:46,091] Trial 35 finished with value: 0.04975072049140669 and parameters: {'iterations': 794, 'learning_rate': 0.0017073405059586826, 'depth': 6, 'l2_leaf_reg': 0.0010427556796508617, 'bootstrap_type': 'Bayesian', 'random_strength': 9.955533116658074e-06, 'bagging_temperature': 3.759329792542367, 'od_type': 'Iter', 'od_wait': 37, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:55:35,712] Trial 36 finished with value: 0.048425437027999244 and parameters: {'iterations': 719, 'learning_rate': 0.0013338988965944528, 'depth': 10, 'l2_leaf_reg': 0.031140314468594173, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00016347156514132417, 'bagging_temperature': 1.4845009532675584, 'od_type': 'Iter', 'od_wait': 33, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:56:15,560] Trial 37 finished with value: 0.04903548814607569 and parameters: {'iterations': 968, 'learning_rate': 0.002195193073757335, 'depth': 9, 'l2_leaf_reg': 0.8950926846020272, 'bootstrap_type': 'Bayesian', 'random_strength': 1.0052892922621313e-06, 'bagging_temperature': 2.299275182008683, 'od_type': 'IncToDec', 'od_wait': 34, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:56:22,087] Trial 38 finished with value: 0.05084460525485411 and parameters: {'iterations': 604, 'learning_rate': 0.0018481134810203876, 'depth': 5, 'l2_leaf_reg': 0.0005689979843034952, 'bootstrap_type': 'Bayesian', 'random_strength': 1.939743571666247e-05, 'bagging_temperature': 4.908246436677717, 'od_type': 'Iter', 'od_wait': 29, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:56:33,398] Trial 39 finished with value: 0.04960346677325031 and parameters: {'iterations': 911, 'learning_rate': 0.0033545703760550195, 'depth': 8, 'l2_leaf_reg': 1.1996108760125428, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00015066032693515072, 'bagging_temperature': 3.762331314079389, 'od_type': 'Iter', 'od_wait': 41, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:56:50,794] Trial 40 finished with value: 0.04939310431874119 and parameters: {'iterations': 1026, 'learning_rate': 0.0012392314265797816, 'depth': 7, 'l2_leaf_reg': 0.000843319329756849, 'bootstrap_type': 'Bayesian', 'random_strength': 8.371950914790935e-08, 'bagging_temperature': 3.104407668052085, 'od_type': 'IncToDec', 'od_wait': 35, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:56:59,438] Trial 41 finished with value: 0.04859372699160654 and parameters: {'iterations': 1242, 'learning_rate': 0.0097322909876524, 'depth': 10, 'l2_leaf_reg': 6.005493564408597e-07, 'bootstrap_type': 'Bayesian', 'random_strength': 4.220024683989887e-06, 'bagging_temperature': 2.172788499264268, 'od_type': 'Iter', 'od_wait': 24, 'loss_function': 'MultiLogloss'}. Best is trial 33 with value: 0.04821507457349012.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:57:12,750] Trial 42 finished with value: 0.047899530891726445 and parameters: {'iterations': 1411, 'learning_rate': 0.006015379212439724, 'depth': 10, 'l2_leaf_reg': 0.00012435060455436673, 'bootstrap_type': 'Bayesian', 'random_strength': 1.2756152905883538e-05, 'bagging_temperature': 1.9744454459953242, 'od_type': 'Iter', 'od_wait': 20, 'loss_function': 'MultiCrossEntropy'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:57:42,308] Trial 43 finished with value: 0.048299219555293774 and parameters: {'iterations': 1412, 'learning_rate': 0.002650361890919766, 'depth': 10, 'l2_leaf_reg': 0.002225202517515035, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0006547503234078923, 'bagging_temperature': 1.3740425869294899, 'od_type': 'Iter', 'od_wait': 20, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:58:09,189] Trial 44 finished with value: 0.04888823442791931 and parameters: {'iterations': 1470, 'learning_rate': 0.0018034974708112607, 'depth': 9, 'l2_leaf_reg': 0.0021502102453989347, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00071975338129878, 'bagging_temperature': 1.3705506962608682, 'od_type': 'Iter', 'od_wait': 43, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:58:14,451] Trial 45 finished with value: 0.04859372699160654 and parameters: {'iterations': 1415, 'learning_rate': 0.01749415598815204, 'depth': 10, 'l2_leaf_reg': 0.00015911961140684257, 'bootstrap_type': 'Bayesian', 'random_strength': 0.0005431109390696291, 'bagging_temperature': 0.9268558485011678, 'od_type': 'Iter', 'od_wait': 16, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:59:33,232] Trial 46 finished with value: 0.04850958200980289 and parameters: {'iterations': 1413, 'learning_rate': 0.004037949910923345, 'depth': 10, 'l2_leaf_reg': 0.012102675762225052, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00011792283760340421, 'bagging_temperature': 0.6980421972907185, 'od_type': 'IncToDec', 'od_wait': 19, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:59:35,029] Trial 47 finished with value: 0.04935103182783937 and parameters: {'iterations': 519, 'learning_rate': 0.08515197005344681, 'depth': 9, 'l2_leaf_reg': 0.023194698408627777, 'bootstrap_type': 'Bayesian', 'random_strength': 1.664647217080111e-05, 'bagging_temperature': 1.8995360945850377, 'od_type': 'Iter', 'od_wait': 29, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:59:38,553] Trial 48 finished with value: 0.05002419168226855 and parameters: {'iterations': 1559, 'learning_rate': 0.0055432655363884566, 'depth': 4, 'l2_leaf_reg': 0.0025131938376425954, 'bootstrap_type': 'Bayesian', 'random_strength': 7.654616465119878e-05, 'bagging_temperature': 3.0397216310144666, 'od_type': 'Iter', 'od_wait': 39, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n",
      "Warning: less than 75% gpu memory available for training. Free: 2566.1875 Total: 32494.125\n",
      "[I 2024-05-22 16:59:39,429] Trial 49 finished with value: 0.04939310431874119 and parameters: {'iterations': 726, 'learning_rate': 0.04503774110646574, 'depth': 6, 'l2_leaf_reg': 0.09075075873537924, 'bootstrap_type': 'Bayesian', 'random_strength': 0.00035743360793224533, 'bagging_temperature': 1.3852654054485563, 'od_type': 'Iter', 'od_wait': 25, 'loss_function': 'MultiLogloss'}. Best is trial 42 with value: 0.047899530891726445.\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=1337)\n",
    "study = optuna.create_study(study_name=\"catboost\", direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  50\n",
      "Best trial:\n",
      "  Value:  0.047899530891726445\n",
      "  Params: \n",
      "    iterations: 1411\n",
      "    learning_rate: 0.006015379212439724\n",
      "    depth: 10\n",
      "    l2_leaf_reg: 0.00012435060455436673\n",
      "    bootstrap_type: Bayesian\n",
      "    random_strength: 1.2756152905883538e-05\n",
      "    bagging_temperature: 1.9744454459953242\n",
      "    od_type: Iter\n",
      "    od_wait: 20\n",
      "    loss_function: MultiCrossEntropy\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6758143\ttest: 0.6766364\tbest: 0.6766364 (0)\ttotal: 231ms\tremaining: 5m 25s\n",
      "1:\tlearn: 0.6587770\ttest: 0.6605332\tbest: 0.6605332 (1)\ttotal: 469ms\tremaining: 5m 30s\n",
      "2:\tlearn: 0.6422147\ttest: 0.6448593\tbest: 0.6448593 (2)\ttotal: 704ms\tremaining: 5m 30s\n",
      "3:\tlearn: 0.6261522\ttest: 0.6297606\tbest: 0.6297606 (3)\ttotal: 934ms\tremaining: 5m 28s\n",
      "4:\tlearn: 0.6101038\ttest: 0.6145738\tbest: 0.6145738 (4)\ttotal: 1.16s\tremaining: 5m 26s\n",
      "5:\tlearn: 0.5951995\ttest: 0.6005239\tbest: 0.6005239 (5)\ttotal: 1.4s\tremaining: 5m 28s\n",
      "6:\tlearn: 0.5797521\ttest: 0.5858151\tbest: 0.5858151 (6)\ttotal: 1.64s\tremaining: 5m 29s\n",
      "7:\tlearn: 0.5657393\ttest: 0.5726938\tbest: 0.5726938 (7)\ttotal: 1.88s\tremaining: 5m 29s\n",
      "8:\tlearn: 0.5518817\ttest: 0.5597522\tbest: 0.5597522 (8)\ttotal: 2.12s\tremaining: 5m 30s\n",
      "9:\tlearn: 0.5374216\ttest: 0.5462356\tbest: 0.5462356 (9)\ttotal: 2.37s\tremaining: 5m 31s\n",
      "10:\tlearn: 0.5244859\ttest: 0.5342135\tbest: 0.5342135 (10)\ttotal: 2.61s\tremaining: 5m 32s\n",
      "11:\tlearn: 0.5114694\ttest: 0.5220304\tbest: 0.5220304 (11)\ttotal: 2.86s\tremaining: 5m 33s\n",
      "12:\tlearn: 0.4985537\ttest: 0.5099044\tbest: 0.5099044 (12)\ttotal: 3.09s\tremaining: 5m 32s\n",
      "13:\tlearn: 0.4860283\ttest: 0.4982607\tbest: 0.4982607 (13)\ttotal: 3.32s\tremaining: 5m 31s\n",
      "14:\tlearn: 0.4745487\ttest: 0.4876175\tbest: 0.4876175 (14)\ttotal: 3.56s\tremaining: 5m 31s\n",
      "15:\tlearn: 0.4627156\ttest: 0.4767526\tbest: 0.4767526 (15)\ttotal: 3.82s\tremaining: 5m 33s\n",
      "16:\tlearn: 0.4517929\ttest: 0.4667774\tbest: 0.4667774 (16)\ttotal: 4.06s\tremaining: 5m 32s\n",
      "17:\tlearn: 0.4411424\ttest: 0.4569173\tbest: 0.4569173 (17)\ttotal: 4.3s\tremaining: 5m 33s\n",
      "18:\tlearn: 0.4305741\ttest: 0.4471499\tbest: 0.4471499 (18)\ttotal: 4.53s\tremaining: 5m 31s\n",
      "19:\tlearn: 0.4200388\ttest: 0.4372858\tbest: 0.4372858 (19)\ttotal: 4.77s\tremaining: 5m 32s\n",
      "20:\tlearn: 0.4105150\ttest: 0.4285211\tbest: 0.4285211 (20)\ttotal: 5s\tremaining: 5m 31s\n",
      "21:\tlearn: 0.4005489\ttest: 0.4192092\tbest: 0.4192092 (21)\ttotal: 5.24s\tremaining: 5m 31s\n",
      "22:\tlearn: 0.3914386\ttest: 0.4107758\tbest: 0.4107758 (22)\ttotal: 5.48s\tremaining: 5m 30s\n",
      "23:\tlearn: 0.3826430\ttest: 0.4027611\tbest: 0.4027611 (23)\ttotal: 5.72s\tremaining: 5m 30s\n",
      "24:\tlearn: 0.3741186\ttest: 0.3949826\tbest: 0.3949826 (24)\ttotal: 5.95s\tremaining: 5m 29s\n",
      "25:\tlearn: 0.3649608\ttest: 0.3865070\tbest: 0.3865070 (25)\ttotal: 6.19s\tremaining: 5m 29s\n",
      "26:\tlearn: 0.3558637\ttest: 0.3781797\tbest: 0.3781797 (26)\ttotal: 6.43s\tremaining: 5m 29s\n",
      "27:\tlearn: 0.3479784\ttest: 0.3710705\tbest: 0.3710705 (27)\ttotal: 6.66s\tremaining: 5m 29s\n",
      "28:\tlearn: 0.3396838\ttest: 0.3634168\tbest: 0.3634168 (28)\ttotal: 6.9s\tremaining: 5m 28s\n",
      "29:\tlearn: 0.3319000\ttest: 0.3562807\tbest: 0.3562807 (29)\ttotal: 7.14s\tremaining: 5m 28s\n",
      "30:\tlearn: 0.3245497\ttest: 0.3496090\tbest: 0.3496090 (30)\ttotal: 7.39s\tremaining: 5m 28s\n",
      "31:\tlearn: 0.3173837\ttest: 0.3430774\tbest: 0.3430774 (31)\ttotal: 7.61s\tremaining: 5m 27s\n",
      "32:\tlearn: 0.3108929\ttest: 0.3372917\tbest: 0.3372917 (32)\ttotal: 7.85s\tremaining: 5m 27s\n",
      "33:\tlearn: 0.3038679\ttest: 0.3309126\tbest: 0.3309126 (33)\ttotal: 8.08s\tremaining: 5m 27s\n",
      "34:\tlearn: 0.2972134\ttest: 0.3249000\tbest: 0.3249000 (34)\ttotal: 8.32s\tremaining: 5m 26s\n",
      "35:\tlearn: 0.2907972\ttest: 0.3191109\tbest: 0.3191109 (35)\ttotal: 8.55s\tremaining: 5m 26s\n",
      "36:\tlearn: 0.2843800\ttest: 0.3132900\tbest: 0.3132900 (36)\ttotal: 8.78s\tremaining: 5m 25s\n",
      "37:\tlearn: 0.2784074\ttest: 0.3079560\tbest: 0.3079560 (37)\ttotal: 9.01s\tremaining: 5m 25s\n",
      "38:\tlearn: 0.2723477\ttest: 0.3025430\tbest: 0.3025430 (38)\ttotal: 9.23s\tremaining: 5m 24s\n",
      "39:\tlearn: 0.2660950\ttest: 0.2969634\tbest: 0.2969634 (39)\ttotal: 9.47s\tremaining: 5m 24s\n",
      "40:\tlearn: 0.2607809\ttest: 0.2922788\tbest: 0.2922788 (40)\ttotal: 9.7s\tremaining: 5m 24s\n",
      "41:\tlearn: 0.2555017\ttest: 0.2876270\tbest: 0.2876270 (41)\ttotal: 9.93s\tremaining: 5m 23s\n",
      "42:\tlearn: 0.2502974\ttest: 0.2829940\tbest: 0.2829940 (42)\ttotal: 10.2s\tremaining: 5m 23s\n",
      "43:\tlearn: 0.2451294\ttest: 0.2784365\tbest: 0.2784365 (43)\ttotal: 10.4s\tremaining: 5m 23s\n",
      "44:\tlearn: 0.2399014\ttest: 0.2738207\tbest: 0.2738207 (44)\ttotal: 10.6s\tremaining: 5m 22s\n",
      "45:\tlearn: 0.2351176\ttest: 0.2696557\tbest: 0.2696557 (45)\ttotal: 10.9s\tremaining: 5m 22s\n",
      "46:\tlearn: 0.2301207\ttest: 0.2652380\tbest: 0.2652380 (46)\ttotal: 11.1s\tremaining: 5m 21s\n",
      "47:\tlearn: 0.2252057\ttest: 0.2608936\tbest: 0.2608936 (47)\ttotal: 11.3s\tremaining: 5m 21s\n",
      "48:\tlearn: 0.2205943\ttest: 0.2568649\tbest: 0.2568649 (48)\ttotal: 11.5s\tremaining: 5m 20s\n",
      "49:\tlearn: 0.2159455\ttest: 0.2528139\tbest: 0.2528139 (49)\ttotal: 11.8s\tremaining: 5m 20s\n",
      "50:\tlearn: 0.2117524\ttest: 0.2491707\tbest: 0.2491707 (50)\ttotal: 12s\tremaining: 5m 19s\n",
      "51:\tlearn: 0.2072924\ttest: 0.2453012\tbest: 0.2453012 (51)\ttotal: 12.2s\tremaining: 5m 19s\n",
      "52:\tlearn: 0.2031581\ttest: 0.2417343\tbest: 0.2417343 (52)\ttotal: 12.5s\tremaining: 5m 19s\n",
      "53:\tlearn: 0.1989512\ttest: 0.2381148\tbest: 0.2381148 (53)\ttotal: 12.7s\tremaining: 5m 18s\n",
      "54:\tlearn: 0.1950829\ttest: 0.2348146\tbest: 0.2348146 (54)\ttotal: 12.9s\tremaining: 5m 18s\n",
      "55:\tlearn: 0.1913024\ttest: 0.2316547\tbest: 0.2316547 (55)\ttotal: 13.1s\tremaining: 5m 17s\n",
      "56:\tlearn: 0.1876206\ttest: 0.2285577\tbest: 0.2285577 (56)\ttotal: 13.4s\tremaining: 5m 17s\n",
      "57:\tlearn: 0.1840935\ttest: 0.2256030\tbest: 0.2256030 (57)\ttotal: 13.6s\tremaining: 5m 17s\n",
      "58:\tlearn: 0.1801532\ttest: 0.2222953\tbest: 0.2222953 (58)\ttotal: 13.9s\tremaining: 5m 17s\n",
      "59:\tlearn: 0.1768768\ttest: 0.2195877\tbest: 0.2195877 (59)\ttotal: 14.1s\tremaining: 5m 17s\n",
      "60:\tlearn: 0.1736679\ttest: 0.2169301\tbest: 0.2169301 (60)\ttotal: 14.3s\tremaining: 5m 17s\n",
      "61:\tlearn: 0.1707315\ttest: 0.2145319\tbest: 0.2145319 (61)\ttotal: 14.6s\tremaining: 5m 17s\n",
      "62:\tlearn: 0.1675756\ttest: 0.2119669\tbest: 0.2119669 (62)\ttotal: 14.8s\tremaining: 5m 16s\n",
      "63:\tlearn: 0.1643815\ttest: 0.2093345\tbest: 0.2093345 (63)\ttotal: 15s\tremaining: 5m 16s\n",
      "64:\tlearn: 0.1615459\ttest: 0.2070440\tbest: 0.2070440 (64)\ttotal: 15.3s\tremaining: 5m 16s\n",
      "65:\tlearn: 0.1586584\ttest: 0.2047439\tbest: 0.2047439 (65)\ttotal: 15.5s\tremaining: 5m 16s\n",
      "66:\tlearn: 0.1559221\ttest: 0.2025402\tbest: 0.2025402 (66)\ttotal: 15.8s\tremaining: 5m 16s\n",
      "67:\tlearn: 0.1533620\ttest: 0.2005038\tbest: 0.2005038 (67)\ttotal: 16s\tremaining: 5m 15s\n",
      "68:\tlearn: 0.1507594\ttest: 0.1984677\tbest: 0.1984677 (68)\ttotal: 16.2s\tremaining: 5m 15s\n",
      "69:\tlearn: 0.1480754\ttest: 0.1963405\tbest: 0.1963405 (69)\ttotal: 16.5s\tremaining: 5m 15s\n",
      "70:\tlearn: 0.1454991\ttest: 0.1943675\tbest: 0.1943675 (70)\ttotal: 16.7s\tremaining: 5m 14s\n",
      "71:\tlearn: 0.1429774\ttest: 0.1924295\tbest: 0.1924295 (71)\ttotal: 16.9s\tremaining: 5m 14s\n",
      "72:\tlearn: 0.1405398\ttest: 0.1905663\tbest: 0.1905663 (72)\ttotal: 17.2s\tremaining: 5m 14s\n",
      "73:\tlearn: 0.1383300\ttest: 0.1888608\tbest: 0.1888608 (73)\ttotal: 17.4s\tremaining: 5m 14s\n",
      "74:\tlearn: 0.1361081\ttest: 0.1871738\tbest: 0.1871738 (74)\ttotal: 17.6s\tremaining: 5m 14s\n",
      "75:\tlearn: 0.1339744\ttest: 0.1855716\tbest: 0.1855716 (75)\ttotal: 17.9s\tremaining: 5m 13s\n",
      "76:\tlearn: 0.1318813\ttest: 0.1839998\tbest: 0.1839998 (76)\ttotal: 18.1s\tremaining: 5m 13s\n",
      "77:\tlearn: 0.1304841\ttest: 0.1830376\tbest: 0.1830376 (77)\ttotal: 18.3s\tremaining: 5m 12s\n",
      "78:\tlearn: 0.1283832\ttest: 0.1814489\tbest: 0.1814489 (78)\ttotal: 18.5s\tremaining: 5m 11s\n",
      "79:\tlearn: 0.1262554\ttest: 0.1798743\tbest: 0.1798743 (79)\ttotal: 18.7s\tremaining: 5m 11s\n",
      "80:\tlearn: 0.1244268\ttest: 0.1785756\tbest: 0.1785756 (80)\ttotal: 19s\tremaining: 5m 11s\n",
      "81:\tlearn: 0.1226984\ttest: 0.1773449\tbest: 0.1773449 (81)\ttotal: 19.2s\tremaining: 5m 11s\n",
      "82:\tlearn: 0.1208665\ttest: 0.1760250\tbest: 0.1760250 (82)\ttotal: 19.5s\tremaining: 5m 11s\n",
      "83:\tlearn: 0.1191232\ttest: 0.1748623\tbest: 0.1748623 (83)\ttotal: 19.7s\tremaining: 5m 11s\n",
      "84:\tlearn: 0.1174246\ttest: 0.1736975\tbest: 0.1736975 (84)\ttotal: 19.9s\tremaining: 5m 10s\n",
      "85:\tlearn: 0.1157982\ttest: 0.1725675\tbest: 0.1725675 (85)\ttotal: 20.2s\tremaining: 5m 10s\n",
      "86:\tlearn: 0.1142586\ttest: 0.1714979\tbest: 0.1714979 (86)\ttotal: 20.4s\tremaining: 5m 10s\n",
      "87:\tlearn: 0.1129067\ttest: 0.1705853\tbest: 0.1705853 (87)\ttotal: 20.6s\tremaining: 5m 10s\n",
      "88:\tlearn: 0.1112429\ttest: 0.1694823\tbest: 0.1694823 (88)\ttotal: 20.9s\tremaining: 5m 10s\n",
      "89:\tlearn: 0.1098559\ttest: 0.1685718\tbest: 0.1685718 (89)\ttotal: 21.1s\tremaining: 5m 9s\n",
      "90:\tlearn: 0.1084303\ttest: 0.1676280\tbest: 0.1676280 (90)\ttotal: 21.3s\tremaining: 5m 9s\n",
      "91:\tlearn: 0.1069613\ttest: 0.1666746\tbest: 0.1666746 (91)\ttotal: 21.6s\tremaining: 5m 9s\n",
      "92:\tlearn: 0.1059596\ttest: 0.1660496\tbest: 0.1660496 (92)\ttotal: 21.7s\tremaining: 5m 7s\n",
      "93:\tlearn: 0.1046207\ttest: 0.1651943\tbest: 0.1651943 (93)\ttotal: 22s\tremaining: 5m 7s\n",
      "94:\tlearn: 0.1032501\ttest: 0.1643761\tbest: 0.1643761 (94)\ttotal: 22.2s\tremaining: 5m 7s\n",
      "95:\tlearn: 0.1018032\ttest: 0.1634749\tbest: 0.1634749 (95)\ttotal: 22.4s\tremaining: 5m 7s\n",
      "96:\tlearn: 0.1005125\ttest: 0.1626988\tbest: 0.1626988 (96)\ttotal: 22.7s\tremaining: 5m 7s\n",
      "97:\tlearn: 0.0993761\ttest: 0.1619757\tbest: 0.1619757 (97)\ttotal: 22.9s\tremaining: 5m 6s\n",
      "98:\tlearn: 0.0981431\ttest: 0.1612367\tbest: 0.1612367 (98)\ttotal: 23.1s\tremaining: 5m 6s\n",
      "99:\tlearn: 0.0969542\ttest: 0.1605564\tbest: 0.1605564 (99)\ttotal: 23.4s\tremaining: 5m 6s\n",
      "100:\tlearn: 0.0958985\ttest: 0.1599329\tbest: 0.1599329 (100)\ttotal: 23.6s\tremaining: 5m 6s\n",
      "101:\tlearn: 0.0948834\ttest: 0.1593191\tbest: 0.1593191 (101)\ttotal: 23.9s\tremaining: 5m 6s\n",
      "102:\tlearn: 0.0937721\ttest: 0.1586642\tbest: 0.1586642 (102)\ttotal: 24.1s\tremaining: 5m 6s\n",
      "103:\tlearn: 0.0926139\ttest: 0.1580472\tbest: 0.1580472 (103)\ttotal: 24.3s\tremaining: 5m 5s\n",
      "104:\tlearn: 0.0915819\ttest: 0.1574836\tbest: 0.1574836 (104)\ttotal: 24.6s\tremaining: 5m 5s\n",
      "105:\tlearn: 0.0905856\ttest: 0.1569283\tbest: 0.1569283 (105)\ttotal: 24.8s\tremaining: 5m 5s\n",
      "106:\tlearn: 0.0900191\ttest: 0.1566348\tbest: 0.1566348 (106)\ttotal: 24.9s\tremaining: 5m 3s\n",
      "107:\tlearn: 0.0891663\ttest: 0.1561631\tbest: 0.1561631 (107)\ttotal: 25.1s\tremaining: 5m 2s\n",
      "108:\tlearn: 0.0882175\ttest: 0.1556625\tbest: 0.1556625 (108)\ttotal: 25.3s\tremaining: 5m 2s\n",
      "109:\tlearn: 0.0871660\ttest: 0.1551151\tbest: 0.1551151 (109)\ttotal: 25.6s\tremaining: 5m 2s\n",
      "110:\tlearn: 0.0862462\ttest: 0.1546512\tbest: 0.1546512 (110)\ttotal: 25.8s\tremaining: 5m 2s\n",
      "111:\tlearn: 0.0854270\ttest: 0.1542568\tbest: 0.1542568 (111)\ttotal: 26.1s\tremaining: 5m 2s\n",
      "112:\tlearn: 0.0845741\ttest: 0.1538209\tbest: 0.1538209 (112)\ttotal: 26.3s\tremaining: 5m 1s\n",
      "113:\tlearn: 0.0837599\ttest: 0.1533771\tbest: 0.1533771 (113)\ttotal: 26.5s\tremaining: 5m 1s\n",
      "114:\tlearn: 0.0830224\ttest: 0.1530272\tbest: 0.1530272 (114)\ttotal: 26.8s\tremaining: 5m 1s\n",
      "115:\tlearn: 0.0822487\ttest: 0.1526624\tbest: 0.1526624 (115)\ttotal: 27s\tremaining: 5m 1s\n",
      "116:\tlearn: 0.0816795\ttest: 0.1524080\tbest: 0.1524080 (116)\ttotal: 27.1s\tremaining: 5m\n",
      "117:\tlearn: 0.0810977\ttest: 0.1521468\tbest: 0.1521468 (117)\ttotal: 27.3s\tremaining: 4m 59s\n",
      "118:\tlearn: 0.0804054\ttest: 0.1518564\tbest: 0.1518564 (118)\ttotal: 27.5s\tremaining: 4m 59s\n",
      "119:\tlearn: 0.0799847\ttest: 0.1516642\tbest: 0.1516642 (119)\ttotal: 27.7s\tremaining: 4m 57s\n",
      "120:\tlearn: 0.0794199\ttest: 0.1514347\tbest: 0.1514347 (120)\ttotal: 27.8s\tremaining: 4m 56s\n",
      "121:\tlearn: 0.0786868\ttest: 0.1511054\tbest: 0.1511054 (121)\ttotal: 28.1s\tremaining: 4m 56s\n",
      "122:\tlearn: 0.0781669\ttest: 0.1509041\tbest: 0.1509041 (122)\ttotal: 28.2s\tremaining: 4m 55s\n",
      "123:\tlearn: 0.0774310\ttest: 0.1506159\tbest: 0.1506159 (123)\ttotal: 28.5s\tremaining: 4m 55s\n",
      "124:\tlearn: 0.0767328\ttest: 0.1503161\tbest: 0.1503161 (124)\ttotal: 28.7s\tremaining: 4m 55s\n",
      "125:\tlearn: 0.0762338\ttest: 0.1501348\tbest: 0.1501348 (125)\ttotal: 28.8s\tremaining: 4m 54s\n",
      "126:\tlearn: 0.0756058\ttest: 0.1499261\tbest: 0.1499261 (126)\ttotal: 29.1s\tremaining: 4m 54s\n",
      "127:\tlearn: 0.0752421\ttest: 0.1497794\tbest: 0.1497794 (127)\ttotal: 29.2s\tremaining: 4m 52s\n",
      "128:\tlearn: 0.0748476\ttest: 0.1496211\tbest: 0.1496211 (128)\ttotal: 29.4s\tremaining: 4m 51s\n",
      "129:\tlearn: 0.0744797\ttest: 0.1494881\tbest: 0.1494881 (129)\ttotal: 29.5s\tremaining: 4m 50s\n",
      "130:\tlearn: 0.0741135\ttest: 0.1493579\tbest: 0.1493579 (130)\ttotal: 29.6s\tremaining: 4m 49s\n",
      "131:\tlearn: 0.0737340\ttest: 0.1492239\tbest: 0.1492239 (131)\ttotal: 29.8s\tremaining: 4m 48s\n",
      "132:\tlearn: 0.0733568\ttest: 0.1490942\tbest: 0.1490942 (132)\ttotal: 29.9s\tremaining: 4m 47s\n",
      "133:\tlearn: 0.0728030\ttest: 0.1488899\tbest: 0.1488899 (133)\ttotal: 30.1s\tremaining: 4m 47s\n",
      "134:\tlearn: 0.0722916\ttest: 0.1486730\tbest: 0.1486730 (134)\ttotal: 30.4s\tremaining: 4m 47s\n",
      "135:\tlearn: 0.0718304\ttest: 0.1484941\tbest: 0.1484941 (135)\ttotal: 30.5s\tremaining: 4m 46s\n",
      "136:\tlearn: 0.0716298\ttest: 0.1484321\tbest: 0.1484321 (136)\ttotal: 30.7s\tremaining: 4m 45s\n",
      "137:\tlearn: 0.0711680\ttest: 0.1482700\tbest: 0.1482700 (137)\ttotal: 30.8s\tremaining: 4m 44s\n",
      "138:\tlearn: 0.0708276\ttest: 0.1481748\tbest: 0.1481748 (138)\ttotal: 31s\tremaining: 4m 43s\n",
      "139:\tlearn: 0.0703234\ttest: 0.1479982\tbest: 0.1479982 (139)\ttotal: 31.2s\tremaining: 4m 43s\n",
      "140:\tlearn: 0.0697052\ttest: 0.1478707\tbest: 0.1478707 (140)\ttotal: 31.4s\tremaining: 4m 43s\n",
      "141:\tlearn: 0.0692894\ttest: 0.1477561\tbest: 0.1477561 (141)\ttotal: 31.6s\tremaining: 4m 42s\n",
      "142:\tlearn: 0.0689709\ttest: 0.1476882\tbest: 0.1476882 (142)\ttotal: 31.7s\tremaining: 4m 41s\n",
      "143:\tlearn: 0.0686696\ttest: 0.1476105\tbest: 0.1476105 (143)\ttotal: 31.8s\tremaining: 4m 40s\n",
      "144:\tlearn: 0.0683650\ttest: 0.1475515\tbest: 0.1475515 (144)\ttotal: 32s\tremaining: 4m 39s\n",
      "145:\tlearn: 0.0679759\ttest: 0.1474422\tbest: 0.1474422 (145)\ttotal: 32.1s\tremaining: 4m 38s\n",
      "146:\tlearn: 0.0676630\ttest: 0.1473390\tbest: 0.1473390 (146)\ttotal: 32.3s\tremaining: 4m 37s\n",
      "147:\tlearn: 0.0672001\ttest: 0.1472226\tbest: 0.1472226 (147)\ttotal: 32.5s\tremaining: 4m 37s\n",
      "148:\tlearn: 0.0667063\ttest: 0.1470572\tbest: 0.1470572 (148)\ttotal: 32.7s\tremaining: 4m 37s\n",
      "149:\tlearn: 0.0664106\ttest: 0.1469810\tbest: 0.1469810 (149)\ttotal: 32.9s\tremaining: 4m 36s\n",
      "150:\tlearn: 0.0661082\ttest: 0.1469046\tbest: 0.1469046 (150)\ttotal: 33s\tremaining: 4m 35s\n",
      "151:\tlearn: 0.0657246\ttest: 0.1468029\tbest: 0.1468029 (151)\ttotal: 33.2s\tremaining: 4m 34s\n",
      "152:\tlearn: 0.0654407\ttest: 0.1467429\tbest: 0.1467429 (152)\ttotal: 33.3s\tremaining: 4m 33s\n",
      "153:\tlearn: 0.0651565\ttest: 0.1466756\tbest: 0.1466756 (153)\ttotal: 33.4s\tremaining: 4m 32s\n",
      "154:\tlearn: 0.0647822\ttest: 0.1466099\tbest: 0.1466099 (154)\ttotal: 33.7s\tremaining: 4m 32s\n",
      "155:\tlearn: 0.0646201\ttest: 0.1465583\tbest: 0.1465583 (155)\ttotal: 33.8s\tremaining: 4m 32s\n",
      "156:\tlearn: 0.0642657\ttest: 0.1465037\tbest: 0.1465037 (156)\ttotal: 34s\tremaining: 4m 31s\n",
      "157:\tlearn: 0.0641078\ttest: 0.1464666\tbest: 0.1464666 (157)\ttotal: 34.2s\tremaining: 4m 31s\n",
      "158:\tlearn: 0.0638530\ttest: 0.1464130\tbest: 0.1464130 (158)\ttotal: 34.3s\tremaining: 4m 30s\n",
      "159:\tlearn: 0.0633686\ttest: 0.1462988\tbest: 0.1462988 (159)\ttotal: 34.5s\tremaining: 4m 30s\n",
      "160:\tlearn: 0.0631225\ttest: 0.1462551\tbest: 0.1462551 (160)\ttotal: 34.7s\tremaining: 4m 29s\n",
      "161:\tlearn: 0.0628794\ttest: 0.1462269\tbest: 0.1462269 (161)\ttotal: 34.8s\tremaining: 4m 28s\n",
      "162:\tlearn: 0.0624543\ttest: 0.1460824\tbest: 0.1460824 (162)\ttotal: 35.1s\tremaining: 4m 28s\n",
      "163:\tlearn: 0.0623014\ttest: 0.1460629\tbest: 0.1460629 (163)\ttotal: 35.2s\tremaining: 4m 27s\n",
      "164:\tlearn: 0.0620055\ttest: 0.1460439\tbest: 0.1460439 (164)\ttotal: 35.4s\tremaining: 4m 27s\n",
      "165:\tlearn: 0.0616094\ttest: 0.1459709\tbest: 0.1459709 (165)\ttotal: 35.6s\tremaining: 4m 27s\n",
      "166:\tlearn: 0.0614710\ttest: 0.1459593\tbest: 0.1459593 (166)\ttotal: 35.7s\tremaining: 4m 26s\n",
      "167:\tlearn: 0.0611645\ttest: 0.1459092\tbest: 0.1459092 (167)\ttotal: 35.9s\tremaining: 4m 25s\n",
      "168:\tlearn: 0.0610164\ttest: 0.1458786\tbest: 0.1458786 (168)\ttotal: 36s\tremaining: 4m 24s\n",
      "169:\tlearn: 0.0607675\ttest: 0.1458229\tbest: 0.1458229 (169)\ttotal: 36.2s\tremaining: 4m 24s\n",
      "170:\tlearn: 0.0605521\ttest: 0.1457877\tbest: 0.1457877 (170)\ttotal: 36.3s\tremaining: 4m 23s\n",
      "171:\tlearn: 0.0603304\ttest: 0.1457798\tbest: 0.1457798 (171)\ttotal: 36.5s\tremaining: 4m 22s\n",
      "172:\tlearn: 0.0602114\ttest: 0.1457882\tbest: 0.1457798 (171)\ttotal: 36.6s\tremaining: 4m 21s\n",
      "173:\tlearn: 0.0600639\ttest: 0.1457972\tbest: 0.1457798 (171)\ttotal: 36.7s\tremaining: 4m 21s\n",
      "174:\tlearn: 0.0598639\ttest: 0.1457734\tbest: 0.1457734 (174)\ttotal: 36.9s\tremaining: 4m 20s\n",
      "175:\tlearn: 0.0596671\ttest: 0.1457903\tbest: 0.1457734 (174)\ttotal: 37s\tremaining: 4m 19s\n",
      "176:\tlearn: 0.0594596\ttest: 0.1457607\tbest: 0.1457607 (176)\ttotal: 37.1s\tremaining: 4m 18s\n",
      "177:\tlearn: 0.0593288\ttest: 0.1457489\tbest: 0.1457489 (177)\ttotal: 37.2s\tremaining: 4m 17s\n",
      "178:\tlearn: 0.0591372\ttest: 0.1457268\tbest: 0.1457268 (178)\ttotal: 37.4s\tremaining: 4m 17s\n",
      "179:\tlearn: 0.0589285\ttest: 0.1457162\tbest: 0.1457162 (179)\ttotal: 37.5s\tremaining: 4m 16s\n",
      "180:\tlearn: 0.0587347\ttest: 0.1457047\tbest: 0.1457047 (180)\ttotal: 37.7s\tremaining: 4m 16s\n",
      "181:\tlearn: 0.0585368\ttest: 0.1456947\tbest: 0.1456947 (181)\ttotal: 37.8s\tremaining: 4m 15s\n",
      "182:\tlearn: 0.0583442\ttest: 0.1456970\tbest: 0.1456947 (181)\ttotal: 38s\tremaining: 4m 14s\n",
      "183:\tlearn: 0.0582167\ttest: 0.1456967\tbest: 0.1456947 (181)\ttotal: 38.1s\tremaining: 4m 13s\n",
      "184:\tlearn: 0.0580247\ttest: 0.1456804\tbest: 0.1456804 (184)\ttotal: 38.2s\tremaining: 4m 13s\n",
      "185:\tlearn: 0.0578234\ttest: 0.1456989\tbest: 0.1456804 (184)\ttotal: 38.3s\tremaining: 4m 12s\n",
      "186:\tlearn: 0.0575306\ttest: 0.1457538\tbest: 0.1456804 (184)\ttotal: 38.6s\tremaining: 4m 12s\n",
      "187:\tlearn: 0.0573496\ttest: 0.1457574\tbest: 0.1456804 (184)\ttotal: 38.7s\tremaining: 4m 11s\n",
      "188:\tlearn: 0.0572282\ttest: 0.1457663\tbest: 0.1456804 (184)\ttotal: 38.8s\tremaining: 4m 11s\n",
      "189:\tlearn: 0.0571074\ttest: 0.1457626\tbest: 0.1456804 (184)\ttotal: 39s\tremaining: 4m 10s\n",
      "190:\tlearn: 0.0568047\ttest: 0.1457800\tbest: 0.1456804 (184)\ttotal: 39.2s\tremaining: 4m 10s\n",
      "191:\tlearn: 0.0564954\ttest: 0.1458350\tbest: 0.1456804 (184)\ttotal: 39.4s\tremaining: 4m 10s\n",
      "192:\tlearn: 0.0562238\ttest: 0.1458541\tbest: 0.1456804 (184)\ttotal: 39.7s\tremaining: 4m 10s\n",
      "193:\tlearn: 0.0559798\ttest: 0.1458447\tbest: 0.1456804 (184)\ttotal: 39.9s\tremaining: 4m 10s\n",
      "194:\tlearn: 0.0557533\ttest: 0.1458592\tbest: 0.1456804 (184)\ttotal: 40.1s\tremaining: 4m 10s\n",
      "195:\tlearn: 0.0556356\ttest: 0.1458046\tbest: 0.1456804 (184)\ttotal: 40.3s\tremaining: 4m 9s\n",
      "196:\tlearn: 0.0553498\ttest: 0.1458113\tbest: 0.1456804 (184)\ttotal: 40.5s\tremaining: 4m 9s\n",
      "197:\tlearn: 0.0552414\ttest: 0.1458095\tbest: 0.1456804 (184)\ttotal: 40.6s\tremaining: 4m 9s\n",
      "198:\tlearn: 0.0550830\ttest: 0.1458231\tbest: 0.1456804 (184)\ttotal: 40.8s\tremaining: 4m 8s\n",
      "199:\tlearn: 0.0549838\ttest: 0.1458304\tbest: 0.1456804 (184)\ttotal: 40.9s\tremaining: 4m 7s\n",
      "200:\tlearn: 0.0547315\ttest: 0.1458373\tbest: 0.1456804 (184)\ttotal: 41.2s\tremaining: 4m 7s\n",
      "201:\tlearn: 0.0546322\ttest: 0.1458435\tbest: 0.1456804 (184)\ttotal: 41.3s\tremaining: 4m 7s\n",
      "202:\tlearn: 0.0544264\ttest: 0.1458671\tbest: 0.1456804 (184)\ttotal: 41.5s\tremaining: 4m 6s\n",
      "203:\tlearn: 0.0543188\ttest: 0.1458967\tbest: 0.1456804 (184)\ttotal: 41.6s\tremaining: 4m 6s\n",
      "204:\tlearn: 0.0541684\ttest: 0.1459003\tbest: 0.1456804 (184)\ttotal: 41.7s\tremaining: 4m 5s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1456803929\n",
      "bestIteration = 184\n",
      "\n",
      "Shrink model to first 185 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(**trial.params, verbose=True)\n",
    "model.fit(train_pool, eval_set=test_pool)\n",
    "pred_labels = model.predict(test_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score  support\n",
      "0              0.714765  0.647416  0.679426    329.0\n",
      "1              0.894897  0.907782  0.901293   2917.0\n",
      "2              0.886076  0.546875  0.676329    128.0\n",
      "3              0.892670  0.821687  0.855709    415.0\n",
      "4              0.767442  0.678082  0.720000    146.0\n",
      "5              0.881835  0.915031  0.898126   3719.0\n",
      "6              0.860051  0.887605  0.873611   1904.0\n",
      "micro avg      0.875194  0.885541  0.880337   9558.0\n",
      "macro avg      0.842534  0.772068  0.800642   9558.0\n",
      "weighted avg   0.874511  0.885541  0.879148   9558.0\n",
      "samples avg    0.859706  0.870417  0.855579   9558.0\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(np.array(meta_dataset_test[\"labels\"].to_list()), pred_labels, output_dict=True)\n",
    "cr = pd.DataFrame(cr).T\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
