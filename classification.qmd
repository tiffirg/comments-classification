# –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏


```{python}
import re
import eli5
import nltk
import emoji
import emosent
import polars as pl
import pandas as pd
import numpy as np
import warnings
import seaborn as sns
from typing import Any
from nltk.corpus import stopwords
from string import punctuation as PUNCT
from datasets import load_dataset, Dataset
from dostoevsky.tokenization import RegexTokenizer
from dostoevsky.models import FastTextSocialNetworkModel
from polyglot.detect import Detector
from polyglot.detect.base import logger as polyglot_logger
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    BertForSequenceClassification,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EvalPrediction,
    EarlyStoppingCallback,
)
from sklearn.metrics import (
    f1_score,
    roc_auc_score,
    accuracy_score,
    multilabel_confusion_matrix,
    confusion_matrix,
    ConfusionMatrixDisplay,
    classification_report,
    precision_score,
    recall_score,
)
import torch
from torch import nn
import matplotlib.pyplot as plt
from functools import partial
from sklearn.utils.class_weight import compute_class_weight
from typing import Any
import plotly.figure_factory as ff
```

–°—Ç–æ–∏—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è TF-IDF –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –º–æ–¥–µ–ª—å:
```{python}
warnings.filterwarnings("ignore")
polyglot_logger.setLevel("ERROR")
nltk.download("stopwords")
RUSSIAN_STOPWORDS = set(stopwords.words("russian"))
```

```{python}
sentiment_tokenizer = RegexTokenizer()
sentiment_model = FastTextSocialNetworkModel(tokenizer=sentiment_tokenizer)
```

## –î–∞—Ç–∞—Å–µ—Ç

```{python}
dataset = pd.read_csv("practice_cleaned.csv")
dataset.head()
```

```{python}
categories = ['–í–∏–¥–µ–æ', '–î–ó', '–õ–æ–Ω–≥—Ä–∏–¥', '–¢–µ—Å—Ç']
```

## Preprocessing

–ü–æ—Å–ª–µ –∑—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã —Ç–∞–∫–∏–µ –¥–µ—Ñ–µ–∫—Ç—ã, –∫–∞–∫: \
 ‚Ä¢ –í–º–µ—Å—Ç–æ —Å—Å—ã–ª–æ–∫ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —É–∫–∞–∑–∞–Ω–æ "–°–°–´–õ–ö–ê". –ë—ã–ª–æ —Ä–µ—à–µ–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å —Å –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º, —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å –¥–ª—è –±—É–¥—É—â–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä NER. –ü–æ –ª–æ–≥–∏–∫–µ —Å—Å—ã–ª–∫–∏ —á–∞—â–µ –≤—Å–µ–≥–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–í–∏–¥–µ–æ". \
 ‚Ä¢ –í –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —ç–º–æ–¥–∑–∏, –ø—Ä–∏—á–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∏–¥–æ–≤. –≠—Ç–æ –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, ":)", —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ —ç–º–æ–¥–∑–∏, –∫ –ø—Ä–∏–º–µ—Ä—É, "üòä", –∞ —Ç–∞–∫–∂–µ –ø–æ–¥–æ–±–Ω—ã–µ —ç—Ç–∏–º "‚ô•". –í–æ-–ø–µ—Ä–≤—ã—Ö –æ–Ω–∏ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è –¥–ª—è LLM, –≤–æ-–≤—Ç–æ—Ä—ã—Ö –∏–∑ –Ω–∏—Ö –∏–∑—ã–º–∞–µ—Ç—Å—è –ø–æ–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞. \
 ‚Ä¢ –¢–∞–∫–∂–µ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ —Ä–µ–¥–∫–æ –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è - —ç—Ç–æ —Ç–æ–∂–µ –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ –µ—â–µ –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫.
 ‚Ä¢ –í –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∏–Ω–æ–≥–¥–∞ –ø–æ–ø–∞–¥–∞—é—Ç—Å—è –∫—É—Å–∫–∏ –∫–æ–¥–∞, –Ω–µ —Ä–µ–¥–∫–æ –≤–∏–¥–Ω—ã –æ—à–∏–±–∫–∏. –¢–∞–∫ –∂–µ –ø–æ–ø–∞–¥–∞—é—Ç—Å—è —Å—Å—ã–ª–∫–∏ –Ω–∞ Telegram –∏ –¥—Ä—É–≥–∏–µ —Å–æ—Ü.—Å–µ—Ç–∏. –ü–æ —ç—Ç–∏–º –ø—Ä–∏—á–∏–Ω–∞–º —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ–≥–∏, –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏, –∫–∞–∫ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ, —Ç–∞–∫ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ. \
 ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤—ã–¥–µ–ª—è—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–≤–∞ –∫–∞–≤—ã—á–∫–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ –∏—Ç–æ–≥—É –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–≤—ã—á–µ–∫ –ø–æ–¥—Ä—è–¥ (2 –∏ –±–æ–ª–µ–µ), –ø–æ—ç—Ç–æ–º—É –≤—Å—ë –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –≤–∏–¥—É –æ–¥–Ω–æ–π –∫–∞–≤—ã—á–∫–∏.\

–û–¥–Ω–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –¥–∞–Ω–Ω—ã—Ö - –Ω–∞–ª–∏—á–∏–µ –∫–æ–¥–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Å–Ω–æ–≤–Ω–æ–º —è–∑—ã–∫ –∫–æ–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ - —Ä—É—Å—Å–∫–∏–π, –ø—Ä–∏ —ç—Ç–æ–º —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∏—Ä—É—é—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –õ–∞–∫–æ–Ω–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ - –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤, —Ç–µ–º —Å–∞–º—ã–º –¥–µ—Ç–µ–∫—Ç–∏—Ä—É—è –Ω–∞–ª–∏—á–∏–µ –∫–æ–¥–∞:

 ‚Ä¢ –í–æ-–ø–µ—Ä–≤—ã—Ö –±—É–¥–µ–º –ø—Ä–æ—Ö–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ \
 ‚Ä¢ –í–æ-–≤—Ç–æ—Ä—ã—Ö –∑–∞–º–µ—Ç–∏–º, —á—Ç–æ —Ä—É—Å—Å–∫–∏–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Å–ª–∞–≤—è–Ω—Å–∫–∏–º —è–∑—ã–∫–∞–º, —Ç–∞–∫ —á—Ç–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä –º–æ–∂–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∏—Ö –≤–º–µ—Å—Ç–æ –Ω–µ–≥–æ \
 ‚Ä¢ –í-—Ç—Ä–µ—Ç—å–∏—Ö –ø–æ—Å—Ç–∞–≤–∏–º –ø–æ—Ä–æ–≥ –ø–æ –∏–Ω–æ—Å–ª–∞–≤—è–Ω—Å–∫–∏–º —è–∑—ã–∫–∞–º - 30% —Ç–µ–∫—Å—Ç–∞, –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–æ–¥–æ–º \
 ‚Ä¢ –í-—á–µ—Ç–≤–µ—Ä—Ç—ã—Ö –±—É–¥–µ–º —É–¥–∞–ª—è—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–æ–ø —Å–ª–æ–≤–∞ - "input", "print" –∏ —Ç.–¥.\

```{python}
test_smiles = [":\)", ":\^\)", ":\(", "=\)", ":o\)",
               ":D", "=D", ":-/", ":/", ":P", "=]",
               ":-—Ä", "8\)", "=O", ":-o", "X-\)",
               "\^_\^", "o_O", "\$_\$", "\^o\^",
               "\^3\^", "\*-\*", "<3", "\^3", ":\^\)"]

def preprocessing_text(text):
  neutral, positive, negative = 0, 0, 0
  emojis = emosent.get_emoji_sentiment_rank_multiple(text)
  amount_emojis = len(emojis)
  if amount_emojis:
    for emoji_symbol in emojis:
      emoji_rank = emoji_symbol["emoji_sentiment_rank"]
      positive += emoji_rank["positive"]
      neutral += emoji_rank["neutral"]
      negative += emoji_rank["negative"]
    neutral, positive, negative = neutral / amount_emojis, positive / amount_emojis, negative / amount_emojis
  text = emoji.replace_emoji(text, replace=' ')
  text = re.sub(fr"{'|'.join(test_smiles)}", ' ', text)
  text = re.sub(r'@[_A-Za-z0-9/\\-]+', ' ', text)
  text = re.sub(r'C:\\{1,2}\S+\.\S+', ' ', text)
  text = re.sub(r'[~_A-Za-z0-9-]+/[~_A-Za-z0-9-/.]+\.[~_A-Za-z0-9-\.]+', ' ', text)
  text = re.sub(r'/[~_A-Za-z0-9-]+/[~_A-Za-z0-9-/][,:]?', ' ', text)
  text = re.sub(r'\([^\S\n]*\)', ' ', text)
  text = re.sub(r'"{2,4}', '"', text)
  text = re.sub("[^\S\n]+", " ", text)
  text = re.sub("–°–°–´–õ–ö–ê", '—Å—Å—ã–ª–∫–∞', text)
  text = rf"{text}"
  text = re.sub(r"\\n", " ", text)
  text = re.sub(r"\\t", " ", text)
  text = text.strip()
  amount_exclamations = 0
  for symbol in text:
    if symbol == "!":
      amount_exclamations += 1
  return text, neutral, positive, negative, amount_exclamations
```

```{python}
def filter_code(text: str, filter_unknown_language=True) -> bool:
    code_stop_words = ["input", "print", "cin", "cout"]
    threshold = 30
    result_text = []
    have_code = False
    for line in text.strip().splitlines():
        foreign_languages_conf, sum_conf = 0, 0
        try:
            detection = Detector(line, quiet=True)
        except Exception as e:
            line = ''.join(x for x in line if x.isprintable())
            detection = Detector(line, quiet=True)
        for language in detection.languages:
            if language.code not in ["ru", "un", "be", "sr", "uk"]:
                foreign_languages_conf += language.confidence
            sum_conf += language.confidence
        if foreign_languages_conf > threshold:
            have_code = True
        else:
            if not filter_unknown_language or sum_conf:
                if all(stop_word not in line for stop_word in code_stop_words):
                    result_text.append(line)
    if have_code:
        return "\n".join(result_text), have_code
    return text, have_code
```

```{python}
#| code-fold: true

def preprocess_frame(frame: pl.DataFrame) -> pl.DataFrame:
    """Filter out empty comments and comments from small categs.

    Args:
        frame (pl.DataFrame): input raw frame.

    Returns:
        pl.DataFrame: clear processed frame.
    """
    original_shape = frame.shape
    frame = frame.filter(
        ~pl.col("–ö–∞—Ç–µ–≥–æ—Ä–∏—è").is_in(
            ["–ö–∞—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã", "–û–±—â–µ–Ω–∏–µ —Å –∫—É—Ä–∞—Ç–æ—Ä–æ–º"]
        )
    )
    frame = frame.filter(~(pl.col("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π").is_null()))
    print(f"Empty comments & Category filtering: {original_shape} -> {frame.shape}")
    return frame
```

–î–æ–±–∞–≤–∏–º –≤ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–∞, –Ω–µ–≥–∞—Ç–∏–≤–∞, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –∞ —Ç–∞–∫–∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è:
```{python}
dataset[["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "Neutral", "Positive", "Negative", "Exclamations"]] = dataset.apply(lambda x: preprocessing_text(x["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"]), axis=1, result_type="expand")
```


–î–æ–±–∞–≤–∏–º –≤ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ñ–∞–∫—Ç –Ω–∞–ª–∏—á–∏—è –∫–æ–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ:
```{python}
dataset[["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "have_code"]] = dataset.apply(lambda x: filter_code(x["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"]), axis=1, result_type="expand")
```

–î–æ–±–∞–≤–∏–º –≤ –¥–∞—Ç–∞—Å–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–∞, –Ω–µ–≥–∞—Ç–∏–≤–∞, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –±–ª–∞–≥–æ–¥–∞—Ä—è NLP:
```{python}
dataset[["Neutral_NLP", "Positive_NLP", "Negative_NLP", "Speech_NLP"]] = [[el["neutral"], el["positive"], el["negative"], el["speech"]] for el in sentiment_model.predict(dataset["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"], k=5)]
```


```{python}
dataset = dataset[dataset["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"] != ""]
```

## Bert Training 
–î–ª—è –æ–±—É—á–µ–Ω–∏—è `BERT` –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –æ—Ç `huggingface` –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\


 ‚Ä¢ –ü–µ—Ä–µ–π—Ç–∏ –æ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ `.csv` –∫ –Ω–∞–±–æ—Ä—É —Ç–µ–Ω–∑–æ—Ä–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º –≤ —Å–µ–±–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–∫–∞—Ç–µ–≥–æ—Ä–∏–π). –≠—Ç–æ –≤—ã–ø–æ–ª–Ω—è—é—Ç —Ñ—É–Ω–∫—Ü–∏–∏ `preprocess_sample()` –∏ `make_dataset()`\
 ‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –≤ `make_training_pipeline()`\
 ‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è - `compute_metrics()`\
 ‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å Focal Loss –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –¥–∏–∑–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤\


```{python}
class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()
```

```{python}
class FocalBert(BertForSequenceClassification):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = nn.MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze(), labels.squeeze())
                else:
                    loss = loss_fct(logits, labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = FocalLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = nn.BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

```

```{python}
#| code-fold: true

def preprocess_frame(frame: pl.DataFrame) -> pl.DataFrame:
    """Filter out empty comments and comments from small categs.

    Args:
        frame (pl.DataFrame): input raw frame.

    Returns:
        pl.DataFrame: clear processed frame.
    """
    original_shape = frame.shape
    frame = frame.filter(
        ~pl.col("–ö–∞—Ç–µ–≥–æ—Ä–∏—è").is_in(
            ["–ö–∞—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤", "–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã", "–û–±—â–µ–Ω–∏–µ —Å –∫—É—Ä–∞—Ç–æ—Ä–æ–º"]
        )
    )
    frame = frame.filter(~(pl.col("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π").is_null()))
    print(f"Empty comments & Category filtering: {original_shape} -> {frame.shape}")
    return frame
```


```{python}
#| code-fold: true

def preprocess_sample(
    sample: dict[str, Any], tokenizer: AutoTokenizer
) -> dict[str, Any]:
    """Encode input raw string to sequence of tokens.
    Also add corresponding labels.

    Args:
        sample (dict[str, Any]): dataset sample w/ <text-label> pair
        tokenizer (AutoTokenizer): model tokenizer

    Returns:
        dict[str, Any]: transformed sample with tokenized text and labels.
    """
    text = sample["text"]
    # –∫–∞–∂–¥—ã–π —Å—ç–º–ø–ª –ø–∞–¥–¥–∏—Ç—Å—è –¥–æ —Å–∞–º–æ–π –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª-—Ç–∏ –≤ —ç—Ç–æ–º –±–∞—Ç—á–µ (padding="max_length")
    # –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –ø–æ—Å–ª-—Ç–∏ 512 (max_length=512), –≤—Å–µ, —á—Ç–æ –¥–ª–∏–Ω–Ω–µ–µ, –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è (truncation=True)
    encoding = tokenizer(text, padding="max_length", truncation=True, max_length=512)
    encoding["labels"] = sample["labels"]
    return encoding
```

```{python}
#| code-fold: true
#| 
def compute_metrics(p: EvalPrediction) -> dict[str, float]:
    """Calculate metrics used on validation step.

    Args:
        p (EvalPrediction): container with predictions and
        ground-truth labels

    Returns:
        dict[str, float]: dictionary with computed labels
    """
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.argmax(preds, axis=1)
    f1 = f1_score(p.label_ids, preds, average="macro")
    acc = accuracy_score(p.label_ids, preds)
    res = {"f1": f1, "accuracy": acc}
    return res
```


```{python}
#| code-fold: true

def make_dataset(
    frame: pl.DataFrame,
    tokenizer: AutoTokenizer,
    label2id: dict[str, int],
    test_size: float = None,
) -> tuple[Dataset, Dataset]:
    """Create huggingface datasets used in training process.

    Args:
        frame (pl.DataFrame): input frame with text data
        tokenizer (AutoTokenizer): model tokenizer
        label2id (dict[str, int]): mapping from category text names
        to digital ids.
        test_size (float, optional): test split share. Defaults to None.

    Returns:
        tuple[Dataset, Dataset]: train & test splits, tokenized, vectorized and batched.
    """
    # –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Å api hf-datasets
    clear_frame = frame.select(
        pl.col("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π").alias("text"), pl.col("–ö–∞—Ç–µ–≥–æ—Ä–∏—è").alias("labels")
    )  

    # –ø–µ—Ä–µ–π–¥–µ–º –æ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫ —á–∏—Å–ª–µ–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º
    clear_frame = clear_frame.with_columns(pl.col("labels").map_dict(label2id))  

    # –∫–∞—Ä—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø-—è –≤ Dataset.map()
    part_prepr = partial(preprocess_sample, tokenizer=tokenizer)

    train_df, test_df = train_test_split(
        clear_frame,
        test_size=test_size,
        random_state=42,
        stratify=clear_frame["labels"],
    )
    train_dataset = Dataset.from_pandas(train_df.to_pandas(), split="train")
    test_dataset = Dataset.from_pandas(test_df.to_pandas(), split="test")
    encoded_train = train_dataset.map(
        part_prepr, batched=True, remove_columns=train_dataset.column_names
    )
    encoded_test = test_dataset.map(
        part_prepr, batched=True, remove_columns=test_dataset.column_names
    )
    encoded_train.set_format("torch")
    encoded_test.set_format("torch")
    return encoded_train, encoded_test
```

```{python}
#| code-fold: true

def make_training_pipeline(
    exp_name: str,
    tokenizer: AutoTokenizer,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    batch_size: int = 32,
    lr: float = 2e-5,
    epochs_num: int = 20,
) -> Trainer:
    """Training process wrapper.

    Args:
        exp_name (str): name of the local folder
        for saving model checkpoints.
        tokenizer (AutoTokenizer): model tokenizer
        train_dataset (Dataset): train dataset split
        eval_dataset (Dataset): test dataset split
        batch_size (int, optional): number of samples
        in sigle batch. Defaults to 32.
        lr (float, optional): model's learning rate. Defaults to 2e-5.
        epochs_num (int, optional):
        number of training iterations. Defaults to 20.

    Returns:
        Trainer: hf training pipeline abstraction class.
    """
    args = TrainingArguments(
        exp_name,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs_num,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        fp16=True,
    )

    model = FocalBert.from_pretrained(
        "cointegrated/rubert-tiny2",
        problem_type="single_label_classification",
        num_labels=len(id2label),
        id2label=id2label,
        label2id=label2id,
    )

    trainer = Trainer(
        model,
        args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
    )
    return trainer
```

```{python}
#| code-fold: true

def predict(logits: torch.Tensor) -> np.ndarray:
    s = torch.nn.Softmax()
    probs = s(torch.tensor(logits))
    return np.argmax(probs)
```

–ù–∞—á–Ω–µ–º –æ–±—É—á–µ–Ω–∏–µ `BERT` –º–æ–¥–µ–ª–∏:

```{python}
BATCH_SIZE = 100
EPOCHS = 20
TEST_SIZE = 0.2
```

```{python}
#| code-fold: true

dataset_polars = pl.from_dataframe(dataset)
tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny2")
t = preprocess_frame(dataset_polars)
print(t["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"].value_counts(sort=True))
label2id = t["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"].unique().sort().to_list()
label2id = dict(zip(label2id, range(len(label2id))))
id2label = {v: k for k, v in label2id.items()}
```

```{{python}}
#| code-fold: true

train_ds, test_ds = make_dataset(t, tokenizer, label2id, TEST_SIZE)
```

```{{python}}
#| code-fold: true

trainer = make_training_pipeline("category_classification", tokenizer, train_ds, test_ds, batch_size=BATCH_SIZE, epochs_num=EPOCHS)
trainer.train()
```

```{{python}}
preds = trainer.predict(test_ds)
preds
```

## Bert Validation 

```{{python}}
preds = trainer.predict(test_ds)
pred_labels = np.apply_along_axis(predict, 1, preds[0])
pred_labels = [id2label[x] for x in pred_labels]
gt_labels = [id2label[x] for x in preds[1]]
cr = classification_report(gt_labels, pred_labels, output_dict=True)
cr = pd.DataFrame(cr).T
print(cr)

cm = confusion_matrix(gt_labels, pred_labels, labels=list(label2id.keys()))
```

```{{python}}
#| code-fold: true

x = list(label2id.keys())
y = list(reversed(label2id.keys()))
fig = ff.create_annotated_heatmap(np.flipud(cm), x=x, y=y, colorscale="Viridis")
fig.update_layout(title_text="Confusion matrix")
fig.add_annotation(
    dict(
        x=0.5,save_model
        text="Predicted value",
        xref="paper",
        yref="paper",
    )
)

fig.add_annotation(
    dict(
        x=-0.16,
        y=0.5,
        showarrow=False,
        text="Real value",
        textangle=-90,
        xref="paper",
        yref="paper",
    )
)

fig["data"][0]["showscale"] = True
fig.show()
```

## Meta Model Training 
–í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç–∞ –º–æ–¥–µ–ª–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Random Forest` —Å –ø—Ä–∏–∑–∞–Ω–∫–∞–º–∏:\
 ‚Ä¢ 4 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏ `BERT`\
 ‚Ä¢ have_code - –µ—Å—Ç—å –ª–∏ –∫–æ–¥ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\
 ‚Ä¢ One hot encoding –ø–æ "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ" –∏ "–§–∞–∫—É–ª—å—Ç–µ—Ç"\
 ‚Ä¢ –û—Ü–µ–Ω–∫–∞\
 ‚Ä¢ 4 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ NLP —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞\
 ‚Ä¢ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ —ç–º–æ–¥–∑–∏ –∏ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—é\


–ó–∞–≥—Ä—É–∑–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ `BERT` –¥–ª—è –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–∏
```{python}
bert_preds = pd.DataFrame(np.loadtxt("all_preds.txt"))
```

```{python}
meta_dataset_pl = t.select(pl.exclude("ID —Å—Ç—É–¥–µ–Ω—Ç–∞", "–¢–µ–≥", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "–°—Ç–∞—Ç—É—Å"))
meta_dataset_pl
```


–û–±—ä–µ–¥–∏–Ω–∏–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–¥–µ–ª–∞–µ–º One hot encoding –¥–ª—è "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ" –∏ "–§–∞–∫—É–ª—å—Ç–µ—Ç"
```{python}
directions = pd.get_dummies(meta_dataset_pl.to_pandas()["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"])
departments = pd.get_dummies(meta_dataset_pl.to_pandas()["–§–∞–∫—É–ª—å—Ç–µ—Ç"])
meta_dataset = meta_dataset_pl.select(pl.exclude("–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–§–∞–∫—É–ª—å—Ç–µ—Ç")).to_pandas()
meta_dataset = pd.concat([meta_dataset, directions, departments, bert_preds], axis=1)
meta_dataset["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] = [label2id[el] for el in meta_dataset["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]]
meta_dataset.columns = meta_dataset.columns.astype(str)
```

```{python}
train_df, test_df = train_test_split(meta_dataset, test_size=0.2, random_state=42, stratify=meta_dataset["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"],
)
X_train, y_train = train_df.loc[:, train_df.columns != '–ö–∞—Ç–µ–≥–æ—Ä–∏—è'], train_df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]
X_test, y_test = test_df.loc[:, test_df.columns != '–ö–∞—Ç–µ–≥–æ—Ä–∏—è'], test_df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]
```

–ü–æ –æ–ø—ã—Ç—É —Ç–∞–∫–æ–π –Ω–∞–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –º–µ—Ç–∞ –º–æ–¥–µ–ª–∏
```{python}
model = RandomForestClassifier(max_depth=10, criterion="gini", n_estimators=150, class_weight="balanced")
```


```{python}
model.fit(X=X_train, y=y_train)
```

```{python}
pred_labels = model.predict(X=X_test)
gt_labels = y_test.values
pred_labels = [id2label[x] for x in pred_labels]
gt_labels = [id2label[x] for x in gt_labels]
```

```{python}
cr = classification_report(gt_labels, pred_labels, output_dict=True)
cr = pd.DataFrame(cr).T
print(cr)

cm = confusion_matrix(gt_labels, pred_labels, labels=list(label2id.keys()))
```

```{python}
#| code-fold: true
x = list(label2id.keys())
y = list(reversed(label2id.keys()))
fig = ff.create_annotated_heatmap(np.flipud(cm), x=x, y=y, colorscale="Viridis")
fig.update_layout(title_text="Confusion matrix")
fig.add_annotation(
    dict(
        x=0.5,
        y=-0.15,
        showarrow=False,
        text="Predicted value",
        xref="paper",
        yref="paper",
    )
)

fig.add_annotation(
    dict(
        x=-0.16,
        y=0.5,
        showarrow=False,
        text="Real value",
        textangle=-90,
        xref="paper",
        yref="paper",
    )
)

fig["data"][0]["showscale"] = True
fig.show()
```

–¢–µ–ø–µ—Ä—å –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –º–µ—Ç–∞ –º–æ–¥–µ–ª—å:
```{python}
eli5.explain_weights(model, target_names=id2label, feature_names=X_train.columns.values)
```